{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0924f5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "Evaluation Metrics are used to assess the performance of train models in machine learning:\n",
    "##### Confusion Matrix: \n",
    "A confusion Matrix summarizes the performance of a classification model. A Confusion matrix consists of:\n",
    "##### True Positive (TP):\n",
    "Instances that are correctly predicted as positive by the model\n",
    "##### True Negative (TN):\n",
    "Instances that are correctly predicted as negative by the model\n",
    "##### False Positive(FP):\n",
    "Instances that are incorrectly predicted as positive by the model when the actual class is negative (Type 1 Error)\n",
    "##### False Negative(FN):\n",
    "Instances that are incorrectly predicted as negative by the model when the actual class is positive (Type 2 Error)\n",
    "\n",
    "Based on the values of the confusion matrix, several evaluation metrics can be derived, including accuracy, precision, recall, and F1-Score, which provide a more comprehensive assessment of the model's performance accross different classes.\n",
    "##### Accuracy: \n",
    "Accuracy measures the overall correctness of predictions  made by a classification model. It is the ration of correctly predicted samples to the total number of samples. Accuracy is calculated as\n",
    "\n",
    "****Accuracy = (TP + TN)/(TP + TN + FP + FN)****\n",
    "\n",
    "##### Precision:\n",
    "Precision focuses on the accuracy of the positive predictions made by a classification model. Precision is calculated as \n",
    "\n",
    "****Precision = TP/(TP + FP)****\n",
    "\n",
    "##### Recall: \n",
    "Or Sensitivy is the true positive rate which measures the propotion of true positive predictions out of the total actual positive instances, Mathematically, recall is calculated as \n",
    "\n",
    "****Recall = TP/(TP + FN)****\n",
    "\n",
    "##### F1-Score:\n",
    "The F1-Score is the harmonic mean of precisino and recall. It provides a balanced measure of a model's performance by considering  both precision and recall. F1-Score can be calculated as:\n",
    "\n",
    "****F1-Score = 2 * (Precision * Recall)/(Precision + Recall)****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596af0e",
   "metadata": {},
   "source": [
    "## Hyperparameters and Tuning\n",
    "Hyperparameters are settings that determine how a machine learning model learns and makes predictions. These parameters are not learned from teh data but are predefined by the user and significantly influence the model's performance and behavior. Properly tuning hyperparameters is essential to optimize a model's performance for a specific task.\n",
    "Hyperparameter tuning is a crucial aspect of machine learning model development. Hyperparameters are settings that we must specify before training the model, and they significantly impact a model's performance. Tuning involves finding the best combination of hyperparameters to optimize a model's performance.\n",
    "The Process typically starts with selecting a range of hyperparameter values, and then various techniques such as grid search, random search, or Bayesian optimization are employed to systematically explore these values. Cross Validation is often used to evaluate model performance for different hyperparameter configurations, ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "## KMeans Clustering \n",
    "KMeans is a iterative unsupervised machine learning algorithm used for clustering data points into K different groups or clusters. It divides the data points into clusters such that ll the points in the same cluster have similar properties. \n",
    "KMeans is a centroid-based algorithm. Each cluster is a associated with a centroid which is the center point of the cluster. The Centroid of the cluster is calculated by taking the mean values of all the data points is a cluster. The k value specifies the number of clusters needed from the operation and has to be selected by the user.\n",
    "\n",
    "K-means clustering starts by randomly initializing K cluster centroids. After assigning each data point to the nearest centroid, the algorithm recalculates the centroids by taking the average position of all data points assigned to each cluster. It continues this process until the centroids stabilize, optimizing the clustering by minimizing the total squared distance within each cluster. \n",
    "\n",
    "The step by step explaination of the Kmeans algorithm is as follows:\n",
    "1. **Initialization:** The algorithm requires the user to specifythe number of clusters (k) needed in the ouput. The algorithm automatically select k initial points from the data as the starting centroids, which represent the centers of each cluster.\n",
    "\n",
    "2. **Cluster Assignment:** Each data point in the dataset is assigned to the nearest centroid. This is done by using a distance function to calculate the distance from each point to each of teh centroids and teh data piont is then assigned to the nearest centroid. Distance functions such as the Euclidean distance are used to calculate the distances. The data points have been grouped into K clusters now.\n",
    "\n",
    "3. **Update Centroids:** Now that we have k clusters, we recalculate the centroids of each cluster by taking the mean value of all the data points in the cluster. \n",
    "\n",
    "4. **Convergence:** After updating centroids, it is possible that some of the data points in the cluster might be closer to some centroids of another cluster. Step 2 and 3 are repeated to assign these data points to updated clusters. The process continues till there are no more changes in the centroids indicating the algorithm has converged. The process can also be stopped if a certain criterion has been met such as the maximum number of iterations allowed. \n",
    "\n",
    "Some of Common applications that we can use clustering on images are:\n",
    "**Image Segmentation** Dividing images into similar regions and color quantizations. By Clustering similar pixels together, it can aid in tasks such as object recognition, image compression. \n",
    "\n",
    "**Image retrieval** Color quantization aims to reduce the number of colors needed to represent an image which is something we can use image clustering for by replacing some colors with color of centroid of its cluster.\n",
    "\n",
    "**Content-Based Image Retrieval** Large image databases can be organized by grouping images with similar properties together. This enables efficient retrieval and browsing of images effectively making it easier to search for specific images. \n",
    "\n",
    "**Image Annotation** Clustering can be employed to automatically categorize and annotate images based on their properties. By grouping visually similar images. It can help in automatically generating tags or annotation for images. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5418b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd852b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 185811069.9587214\n",
      "Distance: 120383869.16633701\n",
      "Distance: 37909526.4141376\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 8/image.jpg')\n",
    "# Reshape the image to a 2D array of pixels\n",
    "pixels = image.reshape(-1, 3).astype(np.float32)\n",
    "\n",
    "#define the criteria for k-means clustering\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "# Set K values\n",
    "k_values = [2, 3, 7]\n",
    "\n",
    "cv2.imshow('K-Mean Segmentation', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Perform k-means clustering for each k value and display  the segmented images.\n",
    "for k in k_values:\n",
    "    # Perform K-Mean Clustering\n",
    "    distances, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    # Convert the centers to integers\n",
    "    centers = np.uint8(centers)\n",
    "    \n",
    "    #Replace each pixel with the corresponding cluster center value\n",
    "    segmented_image = centers[labels.flatten()]\n",
    "    segmented_image = segmented_image.reshape(image.shape)\n",
    "    \n",
    "    cv2.imshow('K Mean Segmentation', segmented_image)\n",
    "    print(\"Distance: {}\".format(distances))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea93344",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (k-NN)\n",
    "k-Nearest Neighbors is a popular supervised learning algorithm that is used for classification and regression application. K-NN predicts the class or valueof a new data point based on the majority or average of its K Nearest neighbors in the training dataset.\n",
    "k-NN is an non parameteric algorithm. Thsi means that it does not make any assumptions about the underlying data and instead makes predictions based on the similarity between the input data points and the train labeled data. K-NN is also an instance based algorithm meaning that it does not use the dataset for training, instead, it used the entire dataset during the prediction phase of the operation. \n",
    "\n",
    "The **k-NN** classification algorthm works by implementing the following steps:\n",
    "1. The user chooses the number for neighbors(represented by K) to be considered for classification. Load the dataset and apply any preprocessing such as feature scaling as per the requirements. \n",
    "\n",
    "2. Split the data into training and test sets. The test set will be used to evaluate the performance of the algorithm.\n",
    "\n",
    "3. For each data point in the dataset, calculate its distance to all data points in the training set. Any distance meteric such as the Euclidean distance or the Manhattan distance can be used for calculating the distance between data points. \n",
    "\n",
    "4. The next step is to select the K nearest neighbors to the data point. The K nearest points to the data point will be selected. \n",
    "\n",
    "5. Among the selected K nearest neighbors the class label is assigned to the data point based on the majority voting in its selected neighbors. \n",
    "\n",
    "6. Step 4 and 5 are repeated for all the data points in the test set. \n",
    "\n",
    "7. The model is then evaluated using various evaluation meterics such as accuracy or precision. Depending on the results obtained, the performance of the model can be improved by changing the K value or the distance metrics used for the operation.\n",
    "\n",
    "## Feature Scaling \n",
    "Feature scaling is a preprocessing technique used in machine learning to standardize or normalize the range of features in a dataset. It aims to bring all features onto a similar scale to avoid bias towards features with larger magnitudes. \n",
    "\n",
    "Commonly used methods for feature scaling are:\n",
    "**Normalization (Min-Max scaling):** Normalization involves scaling each feature value to a range of 0 to 1. Min-Max scaling is a common normalization technique that is implemented by subtracting the minimum value of the feature from each data point and then dividing it by the range (maximum value minus the minimum value):\n",
    "\n",
    "***X_scaled = (X - X_min)/(X_max - X_min)***\n",
    "\n",
    "Where X is original feature value, X_min and X_max are minimum and maximum values of the feature respectively and X_scaled are the updated values in the range between 0 to 1.\n",
    "\n",
    "**Standardization (Z-score normalization):** In this method, each feature is transformed such that it has a mean of 0 and a standard deviation of 1. It is acheived by subtracting the mean of the feature from each data point and then dividing it by standard deviation. Standardization preserves the shape fo the distribution and is suitable for features that have an normal distribution. \n",
    "\n",
    "***X_scaled = (X - mean)/(std_deviation)***\n",
    "\n",
    "Where X is the original feature, X_scaled is the scaled value of the feature with mean value of 0 and a standard deviation of 1. \n",
    "The choice between standardization and normalization depends on the specific requirements of the dataset and the machine learning algorithm being used. \n",
    "\n",
    "## Hyperparameters.\n",
    "In KNN the main hyperparameter is the K value. It represents the number of nearest neighbors to consider for classification and regression. It determines the level of complexity and generalization of the model. A smaller value of K makes the model more sensitive to noise and outliers, while a larger value of K makes the model more biased and less flexible.\n",
    "\n",
    "The distance metric is another hyperparameter that is importand in KNNs algorithms. The choice of distance metric, such as Euclidean distance or the Manhattan distance affects how the neighbors are identified and the similarity between data points. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac4eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cde89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Load the Fashion MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028dc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9626302083333333\n",
      "Test Accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "# Flatten the Image\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#Define the KNN Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#Fit the classifier to training data\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on the validation set\n",
    "y_pred_val = knn.predict(X_val)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy = accuracy_score(Y_val, y_pred_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "#Make predictions on the test set\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "#Calculate the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, y_pred_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5dba3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [4 2 8]\n"
     ]
    }
   ],
   "source": [
    "# Select three random test images\n",
    "indices = np.random.randint(0, len(X_test), size=3)\n",
    "images = X_test[indices]\n",
    "predicted_labels = y_pred_test[indices]\n",
    "\n",
    "# Preprocess the images\n",
    "reshaped_images = [cv2.cvtColor(image.reshape(28, 28), cv2.COLOR_GRAY2BGR) for image in images]\n",
    "\n",
    "# Concatenate the images horizontally\n",
    "concatenated_image = np.hstack(reshaped_images)\n",
    "cv2.imshow(\"Images\", concatenated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae323cb0",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic regression is a popular and widely used algorithm in machine learning used for binary classification of data into categories. While initially developed for binary classification (Two Categories marked as 0 and 1), it can be used to handle multiclass classification using some different strategies. \n",
    "\n",
    "Logistic Regression is a supervised machine learning algorithm that works by modeling the relationship between the input features and the output category variable by assuming a linear relationship.\n",
    "\n",
    "The logistic regression algorithm uses the following steps:\n",
    "1. **Input Features:** Logistic regression takes input features that describe the characteristics or attributes of the data we want to classify. For example, in a dog and cat classification problem, the input features could be extracted from images, sucha as the color distribution, texture patterns or shape features. These features provide information about the distinguishing characteristics of dogs and cats, and they are used as the input variables for the logistic regression model.\n",
    "\n",
    "2. **Initialize Weights:** To perform logistic regression, each input feature vector is multiplied by weight 'w', and the weighted features are summed up. The weighted sum represents the influence of each feature on the prediction. The weights are initialized to small random values or set to zeros and serve as starting points for the training process\n",
    "\n",
    "***Sum over i = 1 to n (wixi + b)***\n",
    "In the logistic regression, a bias term 'b' is also added. The bias term allows the logistic regression model to make predictions even when all the input features are zero.\n",
    "\n",
    "3. **Train the Model:** Now tha tour data is ready, we can proceed to train our logistic regression model. During the training process, the algorithm will try to learn the boundary that will seperate the two classes effectively. The boundary is hyperplane that will divide the feature space into two categories, one for cats and the other for dogs in our case. \n",
    "\n",
    "The algorithm works by adjusting the weights assigned to each input feature interatively to minimize the error between the predicted probabilities and the actual labels. The optimization process tries to find the most optimal values for the weights by minimizing the cost or loss function such as cross entropy loss. Logistic regression commonly uses optimization algorithms such as gradient descent, stochastic gradient descent (SGD) or variants of SGD like mini-batch gradient descent. \n",
    "\n",
    "By adjusting the weights during the training, the logistic regression model learns the importance of each input feature in predicting the probability of an image belonging to a specific class (cat or dog). Ultimately , the trained logistic regression model finds the optimal decision boundary that seperates the cat image from the dog image in the feature space using weights and bias term.\n",
    "\n",
    "4. **Gradient Descent:**  Gradient descent is an iterative optimization algorithm that aims to find the optmal set of parameters (weights) that minimize the cost function associated with logistic regression. \n",
    "\n",
    "In gradient descent, the algorithm starts with an initial set of parameters values and updates them iteratively by taking steps propotional to the negative gradient of the cost function. The gradient tells us the direction in which the cost function increases the most. By taking steps in the opposite direction we move towards the direction where the cost function decreases the most, eventually helping us find the minimum of the cost function. \n",
    "\n",
    "At each iteration, the algorithm computes the gradients of the cost function and then updates the parameters by subtracting a scaled value of the gradients. The learning rate determines the step size taken in each iteration. The process continues until a criterion is met, such as reaching a maximum number of iterations or acheiving a desired level of convergence. \n",
    "\n",
    "5. **Calculate Probabilities:** After training, the model uses the input features and their weights to calculate a weighted sum. The weighted sum is then mapped to a probability value between 0 and 1 using a sigmoid function\n",
    "\n",
    "                        Sigmoid(x) = 1/(1 + exp(-x))\n",
    "\n",
    "6. **Prediction:**  Now that our values have converged in to 0 to 1 range, the model makes class predictions by assigning the most likely label (cat or dog) to the image. If the probability is above 0.5 then the model will assign it to the dog class and if it is below 0.5, the image will be assigned to the cat class. \n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Some of the key hyperparameters in logistic regression are as follows:\n",
    "1. Penalty : The hyperparameter determines the regularization used in logistic regression to prevent the overfitting. It can take different values: \n",
    "\n",
    " ***L1 Regularization***, also known as Lasso Regularization, adds the absolute value of the coefficients as penalty term\n",
    " \n",
    " ***L2 Regularization***, also known as Ridge Regularization, adds the squared magnitude of the coefficients as the penalty term\n",
    " \n",
    "***None***, No regularization required\n",
    "                \n",
    "2. C : This parameter denotes the inverse of regularization strength. It controls the amount of regularization applied on the images. Smaller values of C will result in stronger regularization, while larger values will reduce the amount of regularization in the dataset.\n",
    "\n",
    "3. max_iter: This hyperparameter sets the maximum number of iterations for the algorithm to converge. It determines the maximum number of iterations taken for the algorithm to converge to the optimal solution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0958019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d767973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.86796875\n",
      "Test Accuracy: 0.868125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#INitializes the empty lists for dataset and labels\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "train_folder = \"train\"\n",
    "\n",
    "#Load Images from \"dogs\" folder\n",
    "dog_folder = os.path.join(train_folder, 'dogs')\n",
    "for filename in os.listdir(dog_folder):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image = cv2.imread('train/dogs/'+filename, 0)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (64,64))\n",
    "            k = image.flatten()\n",
    "            dataset.append(k)\n",
    "            labels.append(0) # labels 0 for dog image\n",
    "            \n",
    "#Load images from cats folder\n",
    "cat_folder = os.path.join(train_folder, 'cats')\n",
    "for filename in os.listdir(cat_folder):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image = cv2.imread('train/cats/'+filename, 0)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (64, 64))\n",
    "            k = image.flatten()\n",
    "            dataset.append(k)\n",
    "            labels.append(1) # labels 1 for cat image\n",
    "\n",
    "#Convert the dataset and labels for Numpy arrays\n",
    "dataset = np.array(dataset)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Split the dataset into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset, labels, test_size=0.2)\n",
    "\n",
    "# create a logistic regression model\n",
    "logreg = LogisticRegression(max_iter = 500)\n",
    "\n",
    "# Train the Model\n",
    "logreg.fit(dataset, labels)\n",
    "\n",
    "#Evaluate the model on the training set\n",
    "train_predictions = logreg.predict(X_train)\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "#Evaluate the model on the testing set\n",
    "test_predictions = logreg.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "443986a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat.4922.jpg' 'cat.4974.jpg' 'cat.4674.jpg' 'cat.4502.jpg'\n",
      " 'cat.4967.jpg']\n"
     ]
    }
   ],
   "source": [
    "#Select three random images\n",
    "image_files = np.random.choice(os.listdir(\"test/cats\"), size=5, replace=False)\n",
    "print(image_files)\n",
    "\n",
    "images = []\n",
    "#Iterate over the selected image file\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(\"test/cats\", image_file)\n",
    "    \n",
    "    #Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    #Preprocess the image\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(gray_image, (64, 64))\n",
    "    \n",
    "    flattened_image = image.flatten()\n",
    "    reshaped_image = flattened_image.reshape(1, -1)\n",
    "    \n",
    "    #Make a prediction on the image\n",
    "    predicted_label = logreg.predict(reshaped_image)[0]\n",
    "    \n",
    "    #Get the class name based on the predicted label\n",
    "    class_names = ['dog', 'cat']\n",
    "    predicted_class = class_names[predicted_label]\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.7\n",
    "    \n",
    "    cv2.putText(image, predicted_class, (20, 20), font, font_scale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    #Add image with the predicted class to the list\n",
    "    images.append(image)\n",
    "    \n",
    "output_image = np.hstack(images)\n",
    "\n",
    "cv2.imshow(\"output\", output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f77f8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "#initialize empty lists for dataset and labels\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "#Load CIFAR-10 dataset\n",
    "(X_train, Y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "#Select the desired class\n",
    "selected_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "num_images_per_class = 100\n",
    "num_classes = len(selected_classes)\n",
    "\n",
    "selected_images = []\n",
    "test = []\n",
    "#Iterate over the dataset and extract the desired images\n",
    "for class_idx in selected_classes:\n",
    "    class_images = X_train[Y_train.flatten() == class_idx]\n",
    "    selected_images.extend(class_images[:num_images_per_class])\n",
    "    \n",
    "# Convert the list of selected images to a Numpy Array\n",
    "selected_images = np.array(selected_images)\n",
    "\n",
    "# Reshape the images to a flattened shape\n",
    "flattened_images = selected_images.reshape(-1, np.prod(selected_images.shape[1:]))\n",
    "\n",
    "#Initialize these values to the dataset list created earlier\n",
    "dataset = flattened_images\n",
    "\n",
    "#Initialize labels for each class\n",
    "labels = [0]*1000\n",
    "labels = [i // 100 for i in range(1000)]\n",
    "\n",
    "#Convert the dataset and labels to Numpy arrays\n",
    "dataset = np.array(dataset)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Split the extracted images into Train and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset, labels, test_size=0.2)\n",
    "\n",
    "#Create Logistic regression model\n",
    "logreg = LogisticRegression(C=0.1, max_iter=1000)\n",
    "\n",
    "#Train the Model\n",
    "logreg.fit(dataset, labels)\n",
    "\n",
    "#Evaluate the model on the training set\n",
    "train_predictions = logreg.predict(X_train)\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "print('Train Accuracy:', train_accuracy)\n",
    "\n",
    "#Evaluate the model on testing set\n",
    "test_predictions = logreg.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7fb2496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "# funtion to draw predicted class on the image\n",
    "def draw_predicted_class(image, predicted_class):\n",
    "    class_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    label = class_name[predicted_class]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.7\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "    text_size, _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "    text_x = (image.shape[1] - text_size[0]) // 2\n",
    "    text_y = (image.shape[0] - text_size[1]) // 2\n",
    "    cv2.putText(image, label, (text_x, text_y), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "#Load CIFAR-10 dataset\n",
    "(_, _), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Select one random test image\n",
    "index = np.random.randint(len(X_test))\n",
    "image = X_test[index]\n",
    "true_label = Y_test[index]\n",
    "\n",
    "#Make a prediction on the selected image.\n",
    "selected_image = image.reshape(1, -1)\n",
    "predicted_label = logreg.predict(selected_image)\n",
    "print(predicted_label)\n",
    "\n",
    "# Draw predicted class on the image\n",
    "image_with_label = draw_predicted_class(selected_image, predicted_label[0])\n",
    "\n",
    "cv2.imshow(\"log_final_res.jpg\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae00c0",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "Decision Trees are a type of machine learning algorithm that mimics the way humans make decision by considering multiple factors and conditional control statements like if-else conditions. The decision tree algorithm learns decisions rules from the data to make predictions.\n",
    "\n",
    "Decision trees have tree-like structure consisting of nodes and branches. A decision tree has the following components.\n",
    "***Root Node:***    The starting points of a decision tree. The root node represents the entire dataset and serves as the initial point for branching. \n",
    "\n",
    "***Internal Nodes:***   The Internal nodes in a decision tree correspond to specific features or attributes in the dataset. These are the decision point for various features and conditions in the dataset. Each internal node evaluates the value of a particular feature and decides which branch to follow based on a predefined splitting criterion. \n",
    "\n",
    "***Branches:***    The branches are used to connect the internal nodes in a decision tree. These branches represent the decision and possible outcomes based on the decision from each node in the decision tree. Each branch corresponds to a specific value of the feature and leads to the another node. \n",
    "\n",
    "***Leaf Node:***   Terminal nodes of decision trees. These represent the final predicted outcome of the decision trees. Each leaf node contains the final decision based on the path followed from the root node through the internal nodes. \n",
    "\n",
    "The algorithm uses a splitting criterion to make decision in the decision trees. The criteria determin how to divide the dataset a given node. The criteria used generally is entropy in classification problems and mean squared error in regression problems. \n",
    "\n",
    "##### ENTROPY\n",
    "Entropy is used to measure impurity or uncertainity of a set of data. Entropy measures the amount of randomness in a data. In simpler terms entropy tells us how mixed or diverse the data is with respect to the target classes.\n",
    "\n",
    "Entropy is caluclated by examining the distribution of the different categories within the dataset. If all the objects in the dataset. If all the objects in the dataset belong to the same category, the entropy is low(zero), indicating perfect purity. However, if the objects are evenly distributed among multiple categories, the entropy is high, indicating higher uncertainity or impurity.\n",
    "\n",
    "$E = -\\sum_{i=1}^n p_ilog_2(p_i)$\n",
    "\n",
    "where $p_i$ is the probability of an instance belonging to class i.\n",
    "\n",
    "In the context of decision trees, entropy is used as a criterion for determining the optimal splits during the tree-building process. The goal is to find features or attributes that minimize the entropy and maximuze the purity of each resulting subset.\n",
    "\n",
    "By using entropy as a measure, decision trees can systematically evaluate and choose the best splits to create a tree structure that effectively classifies the data based on the given features.\n",
    "\n",
    "In Image classification, entropy helps the algorithm decide which image features to consider when splitting the data. The goal is to find the features that provide the most information to distinguish between different classes of images. \n",
    "\n",
    "For example, in a dataset of dogs vs cats, the decision tree algorithm looks at the features of these images, such as color, texture, or shape, and calculates the entropy for each feature. The entropy tells us how well a particular feature seperates the dogs from cats. \n",
    "\n",
    "The algorithm begins by selecting the feature that provides the most useful information for distinguishing between the two classes. This feature could be something like color, where images are divided into groups based on similar color. Let's say the algorithm chooses a feature involving the tail length. The algorithm then splits the data based on this feature, creating branches where images with large tail lengths are grouped together. It continues this process, selecting features that bring more clarity to the classification task, such as fur color. The algorithm continues this process, creating tree-like structure where each mode represents a feature and the branches represent different values of that feature.\n",
    "\n",
    "***Pruning:***   Decision trees are prone to overfitting. The goal of pruning is to improve the tree's generalization ability and prevent overfitting , where the tree becomes too specific to the training data and performs poorly, on new unseem data . Pruning is the context of decision trees refer to the process of reducing the complexity of a tree by removing unneccessary branches or nodes. \n",
    "Overfitting: Occurs when a machine learning model learns too much from the training data and performs poorly on new, unseen data due to capturing noise or irrelevant patterns. It reflects an overly complex model that lacks generalization. It is unable to generalize the dataset and just learns the training data too closely resulting in poor results on new data points.\n",
    "\n",
    "Two Type of pruning\n",
    "***Pre-Pruning:*** This approach involves stopping the growth of the tree before it becomes fully expanded. It applies certain conditions or criteria to determine when to stop splitting nodes based on the measures like depth of the tree or minimum number of samples required in a node. \n",
    "\n",
    "***Post-Pruning:*** This method involves building the full decision tree and then selectively removing or collapsing certain branches or nodes. The nodes or branches that do not contribute significantly to the overall accuracy or performance of the tree are pruned or removed. \n",
    "\n",
    "In traditional decision tree algorithms, a single tree is constructed to make predictions based on a set of input features. However, ensemble methods take this concept further by constructing an ensemble, or a group, of decision trees. Each tree in the ensemble independently learns patterms and make predictions and the final prediction is determined by aggregating the outputs of alll the trees. One of the most common ensemble algorithms is the ***Random Forest*** algorithm.\n",
    "\n",
    "### Hyperparameters\n",
    "Decision trees have various hyperparameters that can increase their accuracy helping models achieve better performance. \n",
    "\n",
    "***Criterion:***   The criterion hyperparamter determines the function to be used for measuring the quality of a split at each node of the tree. The possible value for this hyperparameter are:\n",
    "1. Gini: Gini impunity quantifies the level of impunity or disorder in a collection of samples. It is the measure of the probability of misclassifying a randomly chosen element in a set. \n",
    "2. entropy: The information gain based on entropy is used as the criterion\n",
    "\n",
    "\n",
    "***Splitter:***   This hyperparameter determines the strategy used to choose the split at each node. It can take two values:\n",
    "1. best: It selects the best split based on the chosen criterion\n",
    "2. random: It selects the best random split. This is used to add randomness in the model and prevents overfitting.\n",
    "\n",
    "***max_dept:***   This hyperparameter sets the maximum depth of the decision tree. It helps limit the complexity of the model by decreasing the tree sizes and thus prevents overfitting. Setting it to None will allow the tree to grow until end or contain minimum samples defined by ***min_samplles_split***\n",
    "\n",
    "***min_samples_split:*** This hyperparameter sets the minimum number of samples required to split an internal node. If a node has fewer samples than min_samples_split, it will not be split further, effectively creating a leaf node.\n",
    "\n",
    "***min_samples_leaf:***    This hyperparameter sets the minimum number of samples required to be at a leaf node. It defines the minimum sizes of the leaf nodes.\n",
    "\n",
    "***max_features:***    This hyperparameter controls the number of features to consider when looking for the best split at each node. The None value will consider all features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1fad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bcd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "123\n",
      "88\n",
      "798\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_features': 20, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Best Score: 0.7875\n",
      "Training Accuracy: 0.95625\n",
      "Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Set the paths to the image folders\n",
    "folder_paths = ['101_ObjectCategories/airplanes', '101_ObjectCategories/car_side', '101_ObjectCategories/helicopter', '101_ObjectCategories/Motorbikes']\n",
    "\n",
    "#Set the number for bins for the histogram\n",
    "num_bins = 26\n",
    "\n",
    "#initialize lists to store the features and labels\n",
    "features=[]\n",
    "labels = []\n",
    "\n",
    "\n",
    "#Iterate over image folders\n",
    "for folder_index, folder_path in enumerate(folder_paths):\n",
    "    #Get the class label from the folder name\n",
    "    class_label = folder_index\n",
    "    print(len(os.listdir(folder_path)))\n",
    "    #Iterate over the images in the folder\n",
    "    for filename in os.listdir(folder_path)[:50]:\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        gray_image = cv2.resize(gray_image, (200, 200))\n",
    "        # Compute the histogram\n",
    "        histogram = cv2.calcHist([gray_image], [0], None, [num_bins], [0, num_bins])\n",
    "        \n",
    "        #Flatten the Histogram and append it to the features list\n",
    "        features.append(gray_image.flatten())\n",
    "        \n",
    "        # Append the class label to the labels list\n",
    "        labels.append(class_label)\n",
    "        \n",
    "# Convert the lists to Numpy Array\n",
    "features = np.array(features)\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "#Initialize a Random Forest Classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "#Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth':[None, 5, 10, 15],\n",
    "    'min_samples_split':[2, 5, 10],\n",
    "    'min_samples_leaf':[1, 2, 5],\n",
    "    'max_features':[5, 10, 15, 20, 25]\n",
    "}\n",
    "\n",
    "#Perform Grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "#Get the best parameters and best score from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "#Use the best model from Grid search for predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "train_predictions = best_model.predict(X_train)\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "#Calculate Accuracy\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d082f7",
   "metadata": {},
   "source": [
    "Similar to what we did earlier, we load the dataset, preprocess the images and this time we pass the images to DecisionTreeClassifier function. In this code, we do not use the pixel values directly, but instead use the histogram values of the images of our images. This time we use grid search to try to optimize our hyperparameter values. \n",
    "\n",
    "***Grid Search:***    Grid search is a hyperparameter tuning technique that iteratively searches for the best combinations of hyperparameters for training a machine learning model\n",
    "\n",
    "A Grid of predefined hyperparameter value is supplied to the model and the model is trained on the all possible combinations of hyperparameters on the dataset. The combination of hyperparameters providing the best performance is selected as optimal choice\n",
    "\n",
    "## Ensemble Learning\n",
    "\n",
    "Ensemble Learning is a machine learning technique that involves combining multiple individual models to create a stronger and more accurate predictive model.\n",
    "The main idea behind ensemble learning is to create multiple models of varying configurations on the same dataset. Each model is trained independently and generates their own predictions. Then these models are combined to make the final prediction by aggregrating the results from each model using methods such as averaging or weighted averaging.\n",
    "\n",
    "This results in better results than individual models since the collective knowledge of all the models trained under different conditions produces an overall more comprehensive and robust approach. The ensemble combines the strengths and compensates for the weaknesses of the individual models, leading to improved accuracy and generalization to make good predictions. \n",
    "\n",
    "There are two types of ensemble learning methods:\n",
    "\n",
    "***Bagging:***  In bagging multiple models are trained independently on different subsets of the dataset. Each model is trained on a random subset and the results of all the models are combined to come up with the final prediction. Bagging also helps reduce overfitting during training and improves the accuracy of the model. Random Forest is a popular algorithm that uses bagging in its operation\n",
    "\n",
    "***Boosting:*** Boosting is an iterative process than boosts the performance of the model sequentially. First the base model is trained on the entire dataset  and tehn using the outputs of the initial models, subsequent models are trained based on the weaknesses of the base model. Boosting assigns higher weights to the misclassified instances to emphasize their importance in subsequent iterations. The predictions of all the models are then combined through weighted averaging to make the final prediction. Gradient Boosting and AdaBoost are popular boosting algorithms.\n",
    "\n",
    "To Summarize, Bagging combines predictions from independently trained models, while boosting iteratively improves model performance by emphasizeing misclassified instances, both enhancing accuracy and robustness of ensemble models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce22df",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Steps involved in Random Forest Algorithm\n",
    "1. Randomly select a subset of dataset to create multiple samples ensuring diversity in the data for each decision tree. \n",
    "2. Fore each tree, a random subset of features is selected from the available features. The maximum number of features to be considered are specified and a random subset is selected accordingly. \n",
    "3. A decision tree is constructed using the randomly selected data subsets and features and a splitting criteria is used to construct the tree. \n",
    "4. Steps 1 to 2 are repeated till the required amount of trees are constructed forming a random forest.\n",
    "5. To make a prediction, input data is passed through all the trees and each tree produces an output. All the outputs are then combined and a final prediction is reached based on majority voting or averaging. \n",
    "\n",
    "#### Randomness.\n",
    "Random Forest introduces randomness in two ways\n",
    "1. As Mentioned earlier, random forest is a ensemble method that uses bagging. Subsets of training dataset with replacments are used in decision trees. This means that each tree is trained on a slightly different data. This introduces variaions in the dataset effectively introducing randomness and diversity in the dataset. This also helps to reduce overfitting since each tree has its unique training data effectively reducing the correlation between the trees. This helps the trees capture different variations and patterns in the dataset. As a result, the ensemble model is less likely to overfit to any specific pattern or noise present in the training data.\n",
    "2. At each split decision of the tree, a random subset of features is selected randomly for finding the best split. By limiting the features to consider at each split, the algorithm ensures that no single feature dominates the decision making process. This is beneficial because it encourages the ensemble to consider a wider range of features and capture different aspects of the data. Additionally by reducing the influence of individual features, the algorithm becomes less susceptible to noisy or irrelevant features that may exist in the dataset. The feature selection mechanism helps to focus on the most informative and discriminative features, leading to more effective and accurate decision trees.\n",
    "\n",
    "Important features of Random Forest Classifier:\n",
    "\n",
    "***Voting and Aggregation:***    As discussed beforehang, Random Forest Classifier combines predictions from multiple decision trees in the ensemble. Random Forest Classifier uses majority voting for classification tasks and averaging for regression tasks to obtain the final prediction. This voting and aggregation process helps in reducing the impact of individual noisy or biased predictions, leading to more robust and accurate results. \n",
    "\n",
    "***Lesser Overfitting:***    Since the algorithm uses multiple trees with varying subsets of data and features, the algorithm is less prone to overfitting reduces variance and bias by averaging the predictions of multiple trees, it diminishes the influence of outliers and individual tree biases, resulting in more reliable and stable predictions.\n",
    "\n",
    "***Feature Importance:***    Random forest Classifier provides a measure of feature importance, indicating the relevance of each feature in making predictions. The algorithm analyzes the usage frequecy and the impact of each feature across the decision trees. This feature importance analysis aids in identifying the most influential features in the dataset, enabling better outputs.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "***n_estimators:***    This is one of the key parameters in random forests representing the number of decision trees to be created in the ensemble. There is a trade-off between accuracy and the computational time as we increase the number of trees.While the accuracy of the algorithm will increase; the computational time will also increase with time with the number of trees. It is important to find an optimal value for this parameter which results in a good accuracy with acceptable amount of computational time needed for the operation\n",
    "\n",
    "***max_features:***    The max features parameter determines the number of features randomly selected at each split point of the tree. Setting a lower value reduces the correlation between the trees and increases the diversity, while setting is to higher value may lead to more correlated trees. Choosing the appropriate value for max_features is crucial in achieving a balance between model complexity and performance. \n",
    "\n",
    "***max_depth:***    The max depth parameters specifies the maximum depth for each decision tree in the random forest ensemble. A deeper tree with a large depth will allow the algorithm to capture more complex features from the dataset. However increasing the maximum depth also increases the chances of overfitting for the model. Setting an appropriate value for this parameter helps control the complexity of the model and prevents overfitting. \n",
    "\n",
    "***max_leaf_node:***   The max_leaf_nodes hyperparameter in Random Forest classifier determines the maximum  number of leaf nodes allowed in each decision tree. It controls the depth and complexity of the trees and helps prevent overfitting. Setting a lower value restricts the tree growth, while a higher value allows for more complex trees. \n",
    "\n",
    "***criterion:***    The criterion parameter specifies the splitting criterial used to evaluate the quality fo the split in each decision tree. For regression tasks, other metrics such as mean squared error(MSE) or mean absolute error (MAE) are more commonly used as the splitting criteria. The two commonly used criteria are ***gini*** and ***entropy*** for classification tasks. The gini Criterion measures the impurity or the degree of mixture of different classes in a node, while the entropy criterion measures the information gain or the reduction in uncertainty after the split.\n",
    "\n",
    "In the addition to these, there are other hyperparameters such as ***min_samples_split*** and ***min_samples_leaf*** that control the minimum number of samples required to split an internal node or to be considered as a leaf node, respectively. These parameters influence the tree's structure and can be adjusted to balance the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16474464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23af65bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.9689\n",
      "Confusion Matrix:\n",
      "[[ 972    0    0    0    0    2    2    1    3    0]\n",
      " [   0 1123    2    3    0    3    2    0    1    1]\n",
      " [   6    0  998    5    3    1    4    8    7    0]\n",
      " [   0    0   12  967    0   10    0    9    8    4]\n",
      " [   1    0    0    0  957    0    5    0    3   16]\n",
      " [   3    0    2   10    4  861    4    1    4    3]\n",
      " [   7    3    2    0    2    5  936    0    3    0]\n",
      " [   2    5   19    2    0    0    0  987    1   12]\n",
      " [   5    0    4    8    6    4    4    5  929    9]\n",
      " [   6    5    1    9   13    6    1    5    4  959]]\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset from Tensorflow\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#Preprocess the image data\n",
    "X_train = X_train.reshape(-1, 784)/255.0\n",
    "X_test = X_test.reshape(-1, 784)/255.0\n",
    "\n",
    "#Initialize Decision Tree Classifier\n",
    "classifier = RandomForestClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Train out Decision Tree Model\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_predictions = classifier.predict(X_train)\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_predictions = classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy )\n",
    "\n",
    "# Generate Confusion matrix\n",
    "cm  = confusion_matrix(Y_test, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7a46c",
   "metadata": {},
   "source": [
    "We print the classification matrix for the obtained results. The provided matrix represents the confusion matrix obtained from the classification task on the MNIST dataset. The diagonal elements of the matrix represent the correctly classified instances for each class, while the off-diagonal elements indicate misclassification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd6b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9498f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "123\n",
      "88\n",
      "798\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.890625\n"
     ]
    }
   ],
   "source": [
    "#Set the paths to the image folders\n",
    "folder_paths = ['101_ObjectCategories/airplanes', '101_ObjectCategories/car_side', '101_ObjectCategories/helicopter', '101_ObjectCategories/Motorbikes']\n",
    "\n",
    "#Set the number of bins for the histogram\n",
    "num_bins = 256\n",
    "\n",
    "#Initialize lists to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "#Iterate over the image folders\n",
    "for folder_index, folder_path in enumerate(folder_paths):\n",
    "    #Get the class label from the folder name\n",
    "    class_label = folder_index\n",
    "    print(len(os.listdir(folder_path)))\n",
    "    #Iterate over the images in the folder\n",
    "    for filename in os.listdir(folder_path)[:80]:\n",
    "        #Read the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        #Convert the image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #Compute the histogram\n",
    "        histogram = cv2.calcHist([gray_image], [0], None, [num_bins], [0, num_bins])\n",
    "        \n",
    "        #Flatten the histogram and append it to the features list\n",
    "        features.append(histogram.flatten())\n",
    "        \n",
    "        #Append the class label to the labels list\n",
    "        labels.append(class_label)\n",
    "        \n",
    "        \n",
    "#Convert the lists to Numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "#Initialize a Random Forest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators = 200, max_depth = 25, max_features=30)\n",
    "\n",
    "#Train the Classifier\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "#Make the predictions on the training set\n",
    "train_predictions = classifier.predict(X_train)\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "#Make the predictions on the testing set\n",
    "test_predictions = classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "#Display images with predicted class\n",
    "num_images = 8\n",
    "selected_indices = np.random.choice(len(X_test), num_images, replace=False)\n",
    "selected_images = X_test[selected_indices]\n",
    "selected_labels = [folder_paths[label] for label in test_predictions[selected_indices]]\n",
    "\n",
    "for i, image_index in enumerate(selected_indices):\n",
    "    folder_index = int(Y_test[image_index])\n",
    "    folder_path = folder_paths[folder_index]\n",
    "    filename = os.listdir(folder_path)[image_index % 80]\n",
    "    image_path = os.path.join(folder_path, filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Draw the predicted class on the image\n",
    "    cv2.putText(image, str(selected_labels[i]), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(f\"Image {i+1}\", image)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52746f5f",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Support Vector Machines is a supervised machine learning algorithm used for both regression and classifiction tasks, SVMs are a powerful set of algorithm known for their ability to handle complex datasets.\n",
    "\n",
    "SVMs work by finding an optimal hyperplane that best seperates the data points belonging to different classes. In binary classification problems the goal is to find the decision boundary that maximizes the distance between the hyperplane and the nearest data points of each data. \n",
    "\n",
    "The main idea behind SVMs is to transform the input data into a higher dimensional feature space, where it becomes easier to find a hyperplane that linearly seperates that data.\n",
    "\n",
    "Before going into further details about SVMs, let us go through some of the terminologies used in SVM\n",
    "***Hyperplane:***    The hyperplane represents a decision boundary that seperates the classes in SVM. The key characteristics of a hyperplane in SVMs is that it maximizes the margin, which is the distance between the hyperplane and the nearest data points of each class. \n",
    "\n",
    "***Margin:*** Margin refers to the distance between the hyperplane and the support vectors. The margin is calculated as the perpendicular distance from the decision boundary to the closest support vector.\n",
    "\n",
    "***Support Vectors:*** Support Vectors are the data points that lie closest to he hyperplane. During Training, these support vectors have the highest influence on the placement and orientation of the decision boundary. If these support vectors are changed or removed, the boundary position and orientation will change.\n",
    "\n",
    "The detailed steps for SVM are as follows\n",
    "***1.Data Preprocessing:***    First prepare the dataset by applying necessary pre-processing techniques such as data cleaning and feature scaling. Select the input features and the target variables from the dataset and split the dataset into train and test data subsets.\n",
    "\n",
    "***2.Select the SVM Kernel:***    The Kernel function maps the original input data into multi-dimensional feature space. This helps the SVM models an optimal hyperspace that seperates the classes with maximum margin. This kernel transformation helps capture non-linear relationships between the data point. \n",
    "\n",
    "#### Kernel Trick\n",
    "The kernel trick is a technique used in SVM to handle non-linearly seperable data. The basic idea behind the kernel trick is to replace the dot product between two data points in the higher dimensional space with a kernel function.\n",
    "The Kernel function calculates the similarity between two data points in the original feature space and maps them to higher dimensional space implicitly. This allows SVM to find a linear decision boundary in the transformed space, which corresponds to a non-linear decision boundary in the original feature space.\n",
    "\n",
    "The choice of a kernel depends on teh data and the problem in consideration. Some of the popular kernels are:\n",
    "1. Linear Kernel: A Linear Kernel creates a linear decision boundary and is thus helpful in datasets that can be seperated linearly\n",
    "2. Polynomial Kernel: Polynomial functions are used in this kernel which makes the data non linear mapping it into higher dimensional space. The decision boundary can be controlled by changing the degree of polynomial in use.\n",
    "3. Radial Basis Function (RBF) Kernel: This type of kernel transforms the data into an infinite dimensional feature space which allows for more flexible boundary decisions making it a popular algorithm to capture complex non-linear relationships in the data.\n",
    "\n",
    "***Train the SVM Model:***    In the training process, the algorithm tries to find the most optimal hyperplane that maximizes the margin between the classes. This is done by solving a convex optimization problem. The algorithm identifies the support vectors, which are the data points that lie closest to the decision boundary. It calculates the corresponding weights for these support vectors to define the hyperplane. By determining the support vectors and their weights, SVM creates a decision boundary that can accurately classify new, unseen data.\n",
    "\n",
    "***Hyperparameter Tuning:***   The performance of the model is evaluated using various metrics such as accuracy, precision or F1 score. The hyperparameters are fine-tuned to optimize the model performance. One of the main hyperparameters in sVM is the Regularization Parameter(C). SVM uses a regularization parameter C that controls the trade off between maximizing the margin and minimizing the misclassification error. \n",
    "Another Hyperparameter is kernel selection hyperparameter.\n",
    "Gamma is another hyperparameter that defines the influence of each training example on the decision boundary. A smaller gamma value indicated a larger influence range, resulting in smoother decision boundary, while a larger gamma value focuses on closer data points, resulting in a more complex decision boundary.\n",
    "\n",
    "***Evaluation and Prediction:*** The performacne of the model is evaluated using various metrics such as accuracy, precision and F1 score. Once the SVM model is trained and evaluated, it can be deployed to make predictions on new, unseen data. The model maps the new data points into the same feature space as the training data and assigns them to the appropriate class based on their position relative to the decision boundary.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
