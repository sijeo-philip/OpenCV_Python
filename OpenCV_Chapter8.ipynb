{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0924f5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "Evaluation Metrics are used to assess the performance of train models in machine learning:\n",
    "##### Confusion Matrix: \n",
    "A confusion Matrix summarizes the performance of a classification model. A Confusion matrix consists of:\n",
    "##### True Positive (TP):\n",
    "Instances that are correctly predicted as positive by the model\n",
    "##### True Negative (TN):\n",
    "Instances that are correctly predicted as negative by the model\n",
    "##### False Positive(FP):\n",
    "Instances that are incorrectly predicted as positive by the model when the actual class is negative (Type 1 Error)\n",
    "##### False Negative(FN):\n",
    "Instances that are incorrectly predicted as negative by the model when the actual class is positive (Type 2 Error)\n",
    "\n",
    "Based on the values of the confusion matrix, several evaluation metrics can be derived, including accuracy, precision, recall, and F1-Score, which provide a more comprehensive assessment of the model's performance accross different classes.\n",
    "##### Accuracy: \n",
    "Accuracy measures the overall correctness of predictions  made by a classification model. It is the ration of correctly predicted samples to the total number of samples. Accuracy is calculated as\n",
    "\n",
    "****Accuracy = (TP + TN)/(TP + TN + FP + FN)****\n",
    "\n",
    "##### Precision:\n",
    "Precision focuses on the accuracy of the positive predictions made by a classification model. Precision is calculated as \n",
    "\n",
    "****Precision = TP/(TP + FP)****\n",
    "\n",
    "##### Recall: \n",
    "Or Sensitivy is the true positive rate which measures the propotion of true positive predictions out of the total actual positive instances, Mathematically, recall is calculated as \n",
    "\n",
    "****Recall = TP/(TP + FN)****\n",
    "\n",
    "##### F1-Score:\n",
    "The F1-Score is the harmonic mean of precisino and recall. It provides a balanced measure of a model's performance by considering  both precision and recall. F1-Score can be calculated as:\n",
    "\n",
    "****F1-Score = 2 * (Precision * Recall)/(Precision + Recall)****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596af0e",
   "metadata": {},
   "source": [
    "## Hyperparameters and Tuning\n",
    "Hyperparameters are settings that determine how a machine learning model learns and makes predictions. These parameters are not learned from teh data but are predefined by the user and significantly influence the model's performance and behavior. Properly tuning hyperparameters is essential to optimize a model's performance for a specific task.\n",
    "Hyperparameter tuning is a crucial aspect of machine learning model development. Hyperparameters are settings that we must specify before training the model, and they significantly impact a model's performance. Tuning involves finding the best combination of hyperparameters to optimize a model's performance.\n",
    "The Process typically starts with selecting a range of hyperparameter values, and then various techniques such as grid search, random search, or Bayesian optimization are employed to systematically explore these values. Cross Validation is often used to evaluate model performance for different hyperparameter configurations, ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "## KMeans Clustering \n",
    "KMeans is a iterative unsupervised machine learning algorithm used for clustering data points into K different groups or clusters. It divides the data points into clusters such that ll the points in the same cluster have similar properties. \n",
    "KMeans is a centroid-based algorithm. Each cluster is a associated with a centroid which is the center point of the cluster. The Centroid of the cluster is calculated by taking the mean values of all the data points is a cluster. The k value specifies the number of clusters needed from the operation and has to be selected by the user.\n",
    "\n",
    "K-means clustering starts by randomly initializing K cluster centroids. After assigning each data point to the nearest centroid, the algorithm recalculates the centroids by taking the average position of all data points assigned to each cluster. It continues this process until the centroids stabilize, optimizing the clustering by minimizing the total squared distance within each cluster. \n",
    "\n",
    "The step by step explaination of the Kmeans algorithm is as follows:\n",
    "1. **Initialization:** The algorithm requires the user to specifythe number of clusters (k) needed in the ouput. The algorithm automatically select k initial points from the data as the starting centroids, which represent the centers of each cluster.\n",
    "\n",
    "2. **Cluster Assignment:** Each data point in the dataset is assigned to the nearest centroid. This is done by using a distance function to calculate the distance from each point to each of teh centroids and teh data piont is then assigned to the nearest centroid. Distance functions such as the Euclidean distance are used to calculate the distances. The data points have been grouped into K clusters now.\n",
    "\n",
    "3. **Update Centroids:** Now that we have k clusters, we recalculate the centroids of each cluster by taking the mean value of all the data points in the cluster. \n",
    "\n",
    "4. **Convergence:** After updating centroids, it is possible that some of the data points in the cluster might be closer to some centroids of another cluster. Step 2 and 3 are repeated to assign these data points to updated clusters. The process continues till there are no more changes in the centroids indicating the algorithm has converged. The process can also be stopped if a certain criterion has been met such as the maximum number of iterations allowed. \n",
    "\n",
    "Some of Common applications that we can use clustering on images are:\n",
    "**Image Segmentation** Dividing images into similar regions and color quantizations. By Clustering similar pixels together, it can aid in tasks such as object recognition, image compression. \n",
    "\n",
    "**Image retrieval** Color quantization aims to reduce the number of colors needed to represent an image which is something we can use image clustering for by replacing some colors with color of centroid of its cluster.\n",
    "\n",
    "**Content-Based Image Retrieval** Large image databases can be organized by grouping images with similar properties together. This enables efficient retrieval and browsing of images effectively making it easier to search for specific images. \n",
    "\n",
    "**Image Annotation** Clustering can be employed to automatically categorize and annotate images based on their properties. By grouping visually similar images. It can help in automatically generating tags or annotation for images. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5418b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd852b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 185811069.9587214\n",
      "Distance: 120383869.16633701\n",
      "Distance: 37909526.4141376\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 8/image.jpg')\n",
    "# Reshape the image to a 2D array of pixels\n",
    "pixels = image.reshape(-1, 3).astype(np.float32)\n",
    "\n",
    "#define the criteria for k-means clustering\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "# Set K values\n",
    "k_values = [2, 3, 7]\n",
    "\n",
    "cv2.imshow('K-Mean Segmentation', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Perform k-means clustering for each k value and display  the segmented images.\n",
    "for k in k_values:\n",
    "    # Perform K-Mean Clustering\n",
    "    distances, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    # Convert the centers to integers\n",
    "    centers = np.uint8(centers)\n",
    "    \n",
    "    #Replace each pixel with the corresponding cluster center value\n",
    "    segmented_image = centers[labels.flatten()]\n",
    "    segmented_image = segmented_image.reshape(image.shape)\n",
    "    \n",
    "    cv2.imshow('K Mean Segmentation', segmented_image)\n",
    "    print(\"Distance: {}\".format(distances))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea93344",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (k-NN)\n",
    "k-Nearest Neighbors is a popular supervised learning algorithm that is used for classification and regression application. K-NN predicts the class or valueof a new data point based on the majority or average of its K Nearest neighbors in the training dataset.\n",
    "k-NN is an non parameteric algorithm. Thsi means that it does not make any assumptions about the underlying data and instead makes predictions based on the similarity between the input data points and the train labeled data. K-NN is also an instance based algorithm meaning that it does not use the dataset for training, instead, it used the entire dataset during the prediction phase of the operation. \n",
    "\n",
    "The **k-NN** classification algorthm works by implementing the following steps:\n",
    "1. The user chooses the number for neighbors(represented by K) to be considered for classification. Load the dataset and apply any preprocessing such as feature scaling as per the requirements. \n",
    "\n",
    "2. Split the data into training and test sets. The test set will be used to evaluate the performance of the algorithm.\n",
    "\n",
    "3. For each data point in the dataset, calculate its distance to all data points in the training set. Any distance meteric such as the Euclidean distance or the Manhattan distance can be used for calculating the distance between data points. \n",
    "\n",
    "4. The next step is to select the K nearest neighbors to the data point. The K nearest points to the data point will be selected. \n",
    "\n",
    "5. Among the selected K nearest neighbors the class label is assigned to the data point based on the majority voting in its selected neighbors. \n",
    "\n",
    "6. Step 4 and 5 are repeated for all the data points in the test set. \n",
    "\n",
    "7. The model is then evaluated using various evaluation meterics such as accuracy or precision. Depending on the results obtained, the performance of the model can be improved by changing the K value or the distance metrics used for the operation.\n",
    "\n",
    "## Feature Scaling \n",
    "Feature scaling is a preprocessing technique used in machine learning to standardize or normalize the range of features in a dataset. It aims to bring all features onto a similar scale to avoid bias towards features with larger magnitudes. \n",
    "\n",
    "Commonly used methods for feature scaling are:\n",
    "**Normalization (Min-Max scaling):** Normalization involves scaling each feature value to a range of 0 to 1. Min-Max scaling is a common normalization technique that is implemented by subtracting the minimum value of the feature from each data point and then dividing it by the range (maximum value minus the minimum value):\n",
    "\n",
    "***X_scaled = (X - X_min)/(X_max - X_min)***\n",
    "\n",
    "Where X is original feature value, X_min and X_max are minimum and maximum values of the feature respectively and X_scaled are the updated values in the range between 0 to 1.\n",
    "\n",
    "**Standardization (Z-score normalization):** In this method, each feature is transformed such that it has a mean of 0 and a standard deviation of 1. It is acheived by subtracting the mean of the feature from each data point and then dividing it by standard deviation. Standardization preserves the shape fo the distribution and is suitable for features that have an normal distribution. \n",
    "\n",
    "***X_scaled = (X - mean)/(std_deviation)***\n",
    "\n",
    "Where X is the original feature, X_scaled is the scaled value of the feature with mean value of 0 and a standard deviation of 1. \n",
    "The choice between standardization and normalization depends on the specific requirements of the dataset and the machine learning algorithm being used. \n",
    "\n",
    "## Hyperparameters.\n",
    "In KNN the main hyperparameter is the K value. It represents the number of nearest neighbors to consider for classification and regression. It determines the level of complexity and generalization of the model. A smaller value of K makes the model more sensitive to noise and outliers, while a larger value of K makes the model more biased and less flexible.\n",
    "\n",
    "The distance metric is another hyperparameter that is importand in KNNs algorithms. The choice of distance metric, such as Euclidean distance or the Manhattan distance affects how the neighbors are identified and the similarity between data points. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac4eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cde89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Load the Fashion MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028dc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9626302083333333\n",
      "Test Accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "# Flatten the Image\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#Define the KNN Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#Fit the classifier to training data\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on the validation set\n",
    "y_pred_val = knn.predict(X_val)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy = accuracy_score(Y_val, y_pred_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "#Make predictions on the test set\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "#Calculate the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, y_pred_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5dba3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [4 2 8]\n"
     ]
    }
   ],
   "source": [
    "# Select three random test images\n",
    "indices = np.random.randint(0, len(X_test), size=3)\n",
    "images = X_test[indices]\n",
    "predicted_labels = y_pred_test[indices]\n",
    "\n",
    "# Preprocess the images\n",
    "reshaped_images = [cv2.cvtColor(image.reshape(28, 28), cv2.COLOR_GRAY2BGR) for image in images]\n",
    "\n",
    "# Concatenate the images horizontally\n",
    "concatenated_image = np.hstack(reshaped_images)\n",
    "cv2.imshow(\"Images\", concatenated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae323cb0",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic regression is a popular and widely used algorithm in machine learning used for binary classification of data into categories. While initially developed for binary classification (Two Categories marked as 0 and 1), it can be used to handle multiclass classification using some different strategies. \n",
    "\n",
    "Logistic Regression is a supervised machine learning algorithm that works by modeling the relationship between the input features and the output category variable by assuming a linear relationship.\n",
    "\n",
    "The logistic regression algorithm uses the following steps:\n",
    "1. **Input Features:** Logistic regression takes input features that describe the characteristics or attributes of the data we want to classify. For example, in a dog and cat classification problem, the input features could be extracted from images, sucha as the color distribution, texture patterns or shape features. These features provide information about the distinguishing characteristics of dogs and cats, and they are used as the input variables for the logistic regression model.\n",
    "\n",
    "2. **Initialize Weights:** To perform logistic regression, each input feature vector is multiplied by weight 'w', and the weighted features are summed up. The weighted sum represents the influence of each feature on the prediction. The weights are initialized to small random values or set to zeros and serve as starting points for the training process\n",
    "\n",
    "***Sum over i = 1 to n (wixi + b)***\n",
    "In the logistic regression, a bias term 'b' is also added. The bias term allows the logistic regression model to make predictions even when all the input features are zero.\n",
    "\n",
    "3. **Train the Model:** Now tha tour data is ready, we can proceed to train our logistic regression model. During the training process, the algorithm will try to learn the boundary that will seperate the two classes effectively. The boundary is hyperplane that will divide the feature space into two categories, one for cats and the other for dogs in our case. \n",
    "\n",
    "The algorithm works by adjusting the weights assigned to each input feature interatively to minimize the error between the predicted probabilities and the actual labels. The optimization process tries to find the most optimal values for the weights by minimizing the cost or loss function such as cross entropy loss. Logistic regression commonly uses optimization algorithms such as gradient descent, stochastic gradient descent (SGD) or variants of SGD like mini-batch gradient descent. \n",
    "\n",
    "By adjusting the weights during the training, the logistic regression model learns the importance of each input feature in predicting the probability of an image belonging to a specific class (cat or dog). Ultimately , the trained logistic regression model finds the optimal decision boundary that seperates the cat image from the dog image in the feature space using weights and bias term.\n",
    "\n",
    "4. **Gradient Descent:**  Gradient descent is an iterative optimization algorithm that aims to find the optmal set of parameters (weights) that minimize the cost function associated with logistic regression. \n",
    "\n",
    "In gradient descent, the algorithm starts with an initial set of parameters values and updates them iteratively by taking steps propotional to the negative gradient of the cost function. The gradient tells us the direction in which the cost function increases the most. By taking steps in the opposite direction we move towards the direction where the cost function decreases the most, eventually helping us find the minimum of the cost function. \n",
    "\n",
    "At each iteration, the algorithm computes the gradients of the cost function and then updates the parameters by subtracting a scaled value of the gradients. The learning rate determines the step size taken in each iteration. The process continues until a criterion is met, such as reaching a maximum number of iterations or acheiving a desired level of convergence. \n",
    "\n",
    "5. **Calculate Probabilities:** After training, the model uses the input features and their weights to calculate a weighted sum. The weighted sum is then mapped to a probability value between 0 and 1 using a sigmoid function\n",
    "\n",
    "                        Sigmoid(x) = 1/(1 + exp(-x))\n",
    "\n",
    "6. **Prediction:**  Now that our values have converged in to 0 to 1 range, the model makes class predictions by assigning the most likely label (cat or dog) to the image. If the probability is above 0.5 then the model will assign it to the dog class and if it is below 0.5, the image will be assigned to the cat class. \n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Some of the key hyperparameters in logistic regression are as follows:\n",
    "1. Penalty : The hyperparameter determines the regularization used in logistic regression to prevent the overfitting. It can take different values: \n",
    "\n",
    " ***L1 Regularization***, also known as Lasso Regularization, adds the absolute value of the coefficients as penalty term\n",
    " \n",
    " ***L2 Regularization***, also known as Ridge Regularization, adds the squared magnitude of the coefficients as the penalty term\n",
    " \n",
    "***None***, No regularization required\n",
    "                \n",
    "2. C : This parameter denotes the inverse of regularization strength. It controls the amount of regularization applied on the images. Smaller values of C will result in stronger regularization, while larger values will reduce the amount of regularization in the dataset.\n",
    "\n",
    "3. max_iter: This hyperparameter sets the maximum number of iterations for the algorithm to converge. It determines the maximum number of iterations taken for the algorithm to converge to the optimal solution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0958019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d767973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.86796875\n",
      "Test Accuracy: 0.868125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#INitializes the empty lists for dataset and labels\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "train_folder = \"train\"\n",
    "\n",
    "#Load Images from \"dogs\" folder\n",
    "dog_folder = os.path.join(train_folder, 'dogs')\n",
    "for filename in os.listdir(dog_folder):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image = cv2.imread('train/dogs/'+filename, 0)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (64,64))\n",
    "            k = image.flatten()\n",
    "            dataset.append(k)\n",
    "            labels.append(0) # labels 0 for dog image\n",
    "            \n",
    "#Load images from cats folder\n",
    "cat_folder = os.path.join(train_folder, 'cats')\n",
    "for filename in os.listdir(cat_folder):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image = cv2.imread('train/cats/'+filename, 0)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, (64, 64))\n",
    "            k = image.flatten()\n",
    "            dataset.append(k)\n",
    "            labels.append(1) # labels 1 for cat image\n",
    "\n",
    "#Convert the dataset and labels for Numpy arrays\n",
    "dataset = np.array(dataset)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Split the dataset into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset, labels, test_size=0.2)\n",
    "\n",
    "# create a logistic regression model\n",
    "logreg = LogisticRegression(max_iter = 500)\n",
    "\n",
    "# Train the Model\n",
    "logreg.fit(dataset, labels)\n",
    "\n",
    "#Evaluate the model on the training set\n",
    "train_predictions = logreg.predict(X_train)\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "#Evaluate the model on the testing set\n",
    "test_predictions = logreg.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "443986a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat.4922.jpg' 'cat.4974.jpg' 'cat.4674.jpg' 'cat.4502.jpg'\n",
      " 'cat.4967.jpg']\n"
     ]
    }
   ],
   "source": [
    "#Select three random images\n",
    "image_files = np.random.choice(os.listdir(\"test/cats\"), size=5, replace=False)\n",
    "print(image_files)\n",
    "\n",
    "images = []\n",
    "#Iterate over the selected image file\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(\"test/cats\", image_file)\n",
    "    \n",
    "    #Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    #Preprocess the image\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(gray_image, (64, 64))\n",
    "    \n",
    "    flattened_image = image.flatten()\n",
    "    reshaped_image = flattened_image.reshape(1, -1)\n",
    "    \n",
    "    #Make a prediction on the image\n",
    "    predicted_label = logreg.predict(reshaped_image)[0]\n",
    "    \n",
    "    #Get the class name based on the predicted label\n",
    "    class_names = ['dog', 'cat']\n",
    "    predicted_class = class_names[predicted_label]\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.7\n",
    "    \n",
    "    cv2.putText(image, predicted_class, (20, 20), font, font_scale, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    #Add image with the predicted class to the list\n",
    "    images.append(image)\n",
    "    \n",
    "output_image = np.hstack(images)\n",
    "\n",
    "cv2.imshow(\"output\", output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f77f8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "#initialize empty lists for dataset and labels\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "#Load CIFAR-10 dataset\n",
    "(X_train, Y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "#Select the desired class\n",
    "selected_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "num_images_per_class = 100\n",
    "num_classes = len(selected_classes)\n",
    "\n",
    "selected_images = []\n",
    "test = []\n",
    "#Iterate over the dataset and extract the desired images\n",
    "for class_idx in selected_classes:\n",
    "    class_images = X_train[Y_train.flatten() == class_idx]\n",
    "    selected_images.extend(class_images[:num_images_per_class])\n",
    "    \n",
    "# Convert the list of selected images to a Numpy Array\n",
    "selected_images = np.array(selected_images)\n",
    "\n",
    "# Reshape the images to a flattened shape\n",
    "flattened_images = selected_images.reshape(-1, np.prod(selected_images.shape[1:]))\n",
    "\n",
    "#Initialize these values to the dataset list created earlier\n",
    "dataset = flattened_images\n",
    "\n",
    "#Initialize labels for each class\n",
    "labels = [0]*1000\n",
    "labels = [i // 100 for i in range(1000)]\n",
    "\n",
    "#Convert the dataset and labels to Numpy arrays\n",
    "dataset = np.array(dataset)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Split the extracted images into Train and test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset, labels, test_size=0.2)\n",
    "\n",
    "#Create Logistic regression model\n",
    "logreg = LogisticRegression(C=0.1, max_iter=1000)\n",
    "\n",
    "#Train the Model\n",
    "logreg.fit(dataset, labels)\n",
    "\n",
    "#Evaluate the model on the training set\n",
    "train_predictions = logreg.predict(X_train)\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "print('Train Accuracy:', train_accuracy)\n",
    "\n",
    "#Evaluate the model on testing set\n",
    "test_predictions = logreg.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7fb2496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "# funtion to draw predicted class on the image\n",
    "def draw_predicted_class(image, predicted_class):\n",
    "    class_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    label = class_name[predicted_class]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.7\n",
    "    color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "    text_size, _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "    text_x = (image.shape[1] - text_size[0]) // 2\n",
    "    text_y = (image.shape[0] - text_size[1]) // 2\n",
    "    cv2.putText(image, label, (text_x, text_y), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "#Load CIFAR-10 dataset\n",
    "(_, _), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Select one random test image\n",
    "index = np.random.randint(len(X_test))\n",
    "image = X_test[index]\n",
    "true_label = Y_test[index]\n",
    "\n",
    "#Make a prediction on the selected image.\n",
    "selected_image = image.reshape(1, -1)\n",
    "predicted_label = logreg.predict(selected_image)\n",
    "print(predicted_label)\n",
    "\n",
    "# Draw predicted class on the image\n",
    "image_with_label = draw_predicted_class(selected_image, predicted_label[0])\n",
    "\n",
    "cv2.imshow(\"log_final_res.jpg\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae00c0",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1fad8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
