{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0924f5",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "Evaluation Metrics are used to assess the performance of train models in machine learning:\n",
    "##### Confusion Matrix: \n",
    "A confusion Matrix summarizes the performance of a classification model. A Confusion matrix consists of:\n",
    "##### True Positive (TP):\n",
    "Instances that are correctly predicted as positive by the model\n",
    "##### True Negative (TN):\n",
    "Instances that are correctly predicted as negative by the model\n",
    "##### False Positive(FP):\n",
    "Instances that are incorrectly predicted as positive by the model when the actual class is negative (Type 1 Error)\n",
    "##### False Negative(FN):\n",
    "Instances that are incorrectly predicted as negative by the model when the actual class is positive (Type 2 Error)\n",
    "\n",
    "Based on the values of the confusion matrix, several evaluation metrics can be derived, including accuracy, precision, recall, and F1-Score, which provide a more comprehensive assessment of the model's performance accross different classes.\n",
    "##### Accuracy: \n",
    "Accuracy measures the overall correctness of predictions  made by a classification model. It is the ration of correctly predicted samples to the total number of samples. Accuracy is calculated as\n",
    "\n",
    "****Accuracy = (TP + TN)/(TP + TN + FP + FN)****\n",
    "\n",
    "##### Precision:\n",
    "Precision focuses on the accuracy of the positive predictions made by a classification model. Precision is calculated as \n",
    "\n",
    "****Precision = TP/(TP + FP)****\n",
    "\n",
    "##### Recall: \n",
    "Or Sensitivy is the true positive rate which measures the propotion of true positive predictions out of the total actual positive instances, Mathematically, recall is calculated as \n",
    "\n",
    "****Recall = TP/(TP + FN)****\n",
    "\n",
    "##### F1-Score:\n",
    "The F1-Score is the harmonic mean of precisino and recall. It provides a balanced measure of a model's performance by considering  both precision and recall. F1-Score can be calculated as:\n",
    "\n",
    "****F1-Score = 2 * (Precision * Recall)/(Precision + Recall)****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596af0e",
   "metadata": {},
   "source": [
    "## Hyperparameters and Tuning\n",
    "Hyperparameters are settings that determine how a machine learning model learns and makes predictions. These parameters are not learned from teh data but are predefined by the user and significantly influence the model's performance and behavior. Properly tuning hyperparameters is essential to optimize a model's performance for a specific task.\n",
    "Hyperparameter tuning is a crucial aspect of machine learning model development. Hyperparameters are settings that we must specify before training the model, and they significantly impact a model's performance. Tuning involves finding the best combination of hyperparameters to optimize a model's performance.\n",
    "The Process typically starts with selecting a range of hyperparameter values, and then various techniques such as grid search, random search, or Bayesian optimization are employed to systematically explore these values. Cross Validation is often used to evaluate model performance for different hyperparameter configurations, ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "## KMeans Clustering \n",
    "KMeans is a iterative unsupervised machine learning algorithm used for clustering data points into K different groups or clusters. It divides the data points into clusters such that ll the points in the same cluster have similar properties. \n",
    "KMeans is a centroid-based algorithm. Each cluster is a associated with a centroid which is the center point of the cluster. The Centroid of the cluster is calculated by taking the mean values of all the data points is a cluster. The k value specifies the number of clusters needed from the operation and has to be selected by the user.\n",
    "\n",
    "K-means clustering starts by randomly initializing K cluster centroids. After assigning each data point to the nearest centroid, the algorithm recalculates the centroids by taking the average position of all data points assigned to each cluster. It continues this process until the centroids stabilize, optimizing the clustering by minimizing the total squared distance within each cluster. \n",
    "\n",
    "The step by step explaination of the Kmeans algorithm is as follows:\n",
    "1. **Initialization:** The algorithm requires the user to specifythe number of clusters (k) needed in the ouput. The algorithm automatically select k initial points from the data as the starting centroids, which represent the centers of each cluster.\n",
    "\n",
    "2. **Cluster Assignment:** Each data point in the dataset is assigned to the nearest centroid. This is done by using a distance function to calculate the distance from each point to each of teh centroids and teh data piont is then assigned to the nearest centroid. Distance functions such as the Euclidean distance are used to calculate the distances. The data points have been grouped into K clusters now.\n",
    "\n",
    "3. **Update Centroids:** Now that we have k clusters, we recalculate the centroids of each cluster by taking the mean value of all the data points in the cluster. \n",
    "\n",
    "4. **Convergence:** After updating centroids, it is possible that some of the data points in the cluster might be closer to some centroids of another cluster. Step 2 and 3 are repeated to assign these data points to updated clusters. The process continues till there are no more changes in the centroids indicating the algorithm has converged. The process can also be stopped if a certain criterion has been met such as the maximum number of iterations allowed. \n",
    "\n",
    "Some of Common applications that we can use clustering on images are:\n",
    "**Image Segmentation** Dividing images into similar regions and color quantizations. By Clustering similar pixels together, it can aid in tasks such as object recognition, image compression. \n",
    "\n",
    "**Image retrieval** Color quantization aims to reduce the number of colors needed to represent an image which is something we can use image clustering for by replacing some colors with color of centroid of its cluster.\n",
    "\n",
    "**Content-Based Image Retrieval** Large image databases can be organized by grouping images with similar properties together. This enables efficient retrieval and browsing of images effectively making it easier to search for specific images. \n",
    "\n",
    "**Image Annotation** Clustering can be employed to automatically categorize and annotate images based on their properties. By grouping visually similar images. It can help in automatically generating tags or annotation for images. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5418b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd852b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 185811069.9587214\n",
      "Distance: 120383869.16633701\n",
      "Distance: 37909526.4141376\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 8/image.jpg')\n",
    "# Reshape the image to a 2D array of pixels\n",
    "pixels = image.reshape(-1, 3).astype(np.float32)\n",
    "\n",
    "#define the criteria for k-means clustering\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "# Set K values\n",
    "k_values = [2, 3, 7]\n",
    "\n",
    "cv2.imshow('K-Mean Segmentation', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Perform k-means clustering for each k value and display  the segmented images.\n",
    "for k in k_values:\n",
    "    # Perform K-Mean Clustering\n",
    "    distances, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    # Convert the centers to integers\n",
    "    centers = np.uint8(centers)\n",
    "    \n",
    "    #Replace each pixel with the corresponding cluster center value\n",
    "    segmented_image = centers[labels.flatten()]\n",
    "    segmented_image = segmented_image.reshape(image.shape)\n",
    "    \n",
    "    cv2.imshow('K Mean Segmentation', segmented_image)\n",
    "    print(\"Distance: {}\".format(distances))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea93344",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (k-NN)\n",
    "k-Nearest Neighbors is a popular supervised learning algorithm that is used for classification and regression application. K-NN predicts the class or valueof a new data point based on the majority or average of its K Nearest neighbors in the training dataset.\n",
    "k-NN is an non parameteric algorithm. Thsi means that it does not make any assumptions about the underlying data and instead makes predictions based on the similarity between the input data points and the train labeled data. K-NN is also an instance based algorithm meaning that it does not use the dataset for training, instead, it used the entire dataset during the prediction phase of the operation. \n",
    "\n",
    "The **k-NN** classification algorthm works by implementing the following steps:\n",
    "1. The user chooses the number for neighbors(represented by K) to be considered for classification. Load the dataset and apply any preprocessing such as feature scaling as per the requirements. \n",
    "\n",
    "2. Split the data into training and test sets. The test set will be used to evaluate the performance of the algorithm.\n",
    "\n",
    "3. For each data point in the dataset, calculate its distance to all data points in the training set. Any distance meteric such as the Euclidean distance or the Manhattan distance can be used for calculating the distance between data points. \n",
    "\n",
    "4. The next step is to select the K nearest neighbors to the data point. The K nearest points to the data point will be selected. \n",
    "\n",
    "5. Among the selected K nearest neighbors the class label is assigned to the data point based on the majority voting in its selected neighbors. \n",
    "\n",
    "6. Step 4 and 5 are repeated for all the data points in the test set. \n",
    "\n",
    "7. The model is then evaluated using various evaluation meterics such as accuracy or precision. Depending on the results obtained, the performance of the model can be improved by changing the K value or the distance metrics used for the operation.\n",
    "\n",
    "## Feature Scaling \n",
    "Feature scaling is a preprocessing technique used in machine learning to standardize or normalize the range of features in a dataset. It aims to bring all features onto a similar scale to avoid bias towards features with larger magnitudes. \n",
    "\n",
    "Commonly used methods for feature scaling are:\n",
    "**Normalization (Min-Max scaling):** Normalization involves scaling each feature value to a range of 0 to 1. Min-Max scaling is a common normalization technique that is implemented by subtracting the minimum value of the feature from each data point and then dividing it by the range (maximum value minus the minimum value):\n",
    "\n",
    "***X_scaled = (X - X_min)/(X_max - X_min)***\n",
    "\n",
    "Where X is original feature value, X_min and X_max are minimum and maximum values of the feature respectively and X_scaled are the updated values in the range between 0 to 1.\n",
    "\n",
    "**Standardization (Z-score normalization):** In this method, each feature is transformed such that it has a mean of 0 and a standard deviation of 1. It is acheived by subtracting the mean of the feature from each data point and then dividing it by standard deviation. Standardization preserves the shape fo the distribution and is suitable for features that have an normal distribution. \n",
    "\n",
    "***X_scaled = (X - mean)/(std_deviation)***\n",
    "\n",
    "Where X is the original feature, X_scaled is the scaled value of the feature with mean value of 0 and a standard deviation of 1. \n",
    "The choice between standardization and normalization depends on the specific requirements of the dataset and the machine learning algorithm being used. \n",
    "\n",
    "## Hyperparameters.\n",
    "In KNN the main hyperparameter is the K value. It represents the number of nearest neighbors to consider for classification and regression. It determines the level of complexity and generalization of the model. A smaller value of K makes the model more sensitive to noise and outliers, while a larger value of K makes the model more biased and less flexible.\n",
    "\n",
    "The distance metric is another hyperparameter that is importand in KNNs algorithms. The choice of distance metric, such as Euclidean distance or the Manhattan distance affects how the neighbors are identified and the similarity between data points. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac4eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cde89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Load the Fashion MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028dc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9626302083333333\n",
      "Test Accuracy: 0.9615\n"
     ]
    }
   ],
   "source": [
    "# Flatten the Image\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#Define the KNN Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#Fit the classifier to training data\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on the validation set\n",
    "y_pred_val = knn.predict(X_val)\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "val_accuracy = accuracy_score(Y_val, y_pred_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "#Make predictions on the test set\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "#Calculate the test accuracy\n",
    "test_accuracy = accuracy_score(Y_test, y_pred_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5dba3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [4 2 8]\n"
     ]
    }
   ],
   "source": [
    "# Select three random test images\n",
    "indices = np.random.randint(0, len(X_test), size=3)\n",
    "images = X_test[indices]\n",
    "predicted_labels = y_pred_test[indices]\n",
    "\n",
    "# Preprocess the images\n",
    "reshaped_images = [cv2.cvtColor(image.reshape(28, 28), cv2.COLOR_GRAY2BGR) for image in images]\n",
    "\n",
    "# Concatenate the images horizontally\n",
    "concatenated_image = np.hstack(reshaped_images)\n",
    "cv2.imshow(\"Images\", concatenated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716088a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958019f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
