{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efd8751",
   "metadata": {},
   "source": [
    "## Edges and Contours\n",
    "### Filters for Image Gradients\n",
    "\n",
    "####  Sobel Filters\n",
    "######  Sobel filters are very commonly used type of spatial filter that calculate the gradients in an image by performing convolution operations. They are primarily used for edge detection due to their simplicity and effectiveness in capturing the edge information. \n",
    "###### The Sobel filter consists of two types of kernels, one for computing the image gradients in the horizontal or the X direction and the other for computing the gradients in vertical or the Y direction .\n",
    "###### The SobelX kernel calculates the image gradients in the X direction and emphasizes the vertical edges in the image. The SobelX kernel enhances vertical edges by assigning negative values to one side and positive values to the other side. This creates a larger difference in values , effectively enhancing the vertical edges in the image. This arrangement accentuates the contrast between the two sides of the vertical edges. Similarly SobelY operator calculates the image gradients in the Y direction and emphasizes the horizontal edges in an image. The SobelY kernel assigns negative and positive values but in the horizontal edges in the image, thus enhancing the contrast between the two sides of the horizontal edges. Together both of the kernels make an effective gradient computation mechanism effectively calculating the gradient magnitude and orientation in the image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713c28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread(\"Images/Input Images/Chapter 7/1.png\", cv2.IMREAD_GRAYSCALE)\n",
    "#Compute the gradient along x and y directions\n",
    "gradient_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "gradient_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "cv2.imshow(\"Original\", image)\n",
    "cv2.imshow(\"Gradient X\", gradient_x)\n",
    "cv2.imshow(\"Gradient Y\", gradient_y)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb4f4f",
   "metadata": {},
   "source": [
    "### Scharr Operator\n",
    "##### The Scharr Operators are a variation of the Sobel operators used for gradient calcualation. They are used to provide a more accurate and rotationally symmetric gradient estimate when compared to the Sobel Operators. Scharr Operators are limited to 3x3 kernel size. Similar to Sobel operators we have seperate Scharr Kernelss for the X and Y direction. The Scharr kernels provide emphasis on the central rows and columns compared to the Sobel operators, which helps in acheiving better rotational symmetry preserving more high frequency information in the gradient estimation. Sobel operators and Scharr operators are both used for finding image gradient, However Scharr operators are sometimes preferred over Sobel operators for a couple of reasons, even though they serve a similar purpose.\n",
    "##### Improved Sensitivity:\n",
    "Scharr operators are more sensitive to subtle changes in image gradients compared to Sobel operators. They can detect edges and details that sobel might miss, making them  abetter choice wehen you need precise edge detection. \n",
    "\n",
    "##### Better rotation invariance: \n",
    "Sobel operators are sensitive to the orientation of the edges, which means they might perform differently depending on whether the edge is horizontal, vertical, or diagonal. Scharr operators are designed to be more rotationally invariant meaning they perform consistently accross various edge orientation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b11068",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 7/image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Compute Scharr X gradient using the cv2.Scharr function\n",
    "gradient_x = cv2.Scharr(image, cv2.CV_32F, 1, 0)\n",
    "\n",
    "#Compute Scharr-like Y gradient using the cv2.Sobel function\n",
    "gradient_y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=-1)\n",
    "\n",
    "cv2.imshow(\"Original\", image)\n",
    "cv2.imshow(\"Gradient X\", gradient_x)\n",
    "cv2.imshow(\"Gradient Y\", gradient_y)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a2512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"Images/Input Images/Chapter 7/objects.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Define Scharr-like kernels\n",
    "scharr_x = np.array([[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]], dtype=np.float32)\n",
    "scharr_y = np.array([[-3, -10, -3], [0, 0, 0], [3, 10, 3]], dtype=np.float32)\n",
    "\n",
    "#Compute Scharr like gradients\n",
    "gradient_x = cv2.filter2D(image, cv2.CV_32F, scharr_x)\n",
    "gradient_y = cv2.filter2D(image, cv2.CV_32F, scharr_y)\n",
    "\n",
    "gradient = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "\n",
    "cv2.imshow(\"Original\", image)\n",
    "\n",
    "cv2.imshow(\"Gradient X\", gradient_x)\n",
    "cv2.imshow(\"Gradient Y\", gradient_y)\n",
    "cv2.imshow(\"Gradient\", gradient)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b644ee",
   "metadata": {},
   "source": [
    "### Laplacian Operators\n",
    "Laplacian is another operator used to compute the gradients in an image to detect edges and regions of high intensity changes in an image. \n",
    "The Laplacian operator is a second-order derivative operator that measures the rate of change of intensity in the image. First order filters identify edges in an image by detecting local maximum or minimum values. In contrast, the Laplacian operator detects edges at points of inflection, which occur when the intensity value transitions from negative to positive or vice versa\n",
    "\n",
    "The Laplacian operator is represented by 3x3 kernel and central element fo the kernel is assigned a negative value (-4 or -8) and the surrounding elements have positive values (1 or 2). This configuration enhances the edges and intensity transitions in the image.\n",
    "The Laplacian operator not only detects edges in an image but also provides additional information about the nature of these edges. It classifies edges into two types: inward edges and outward edges. Inward edges are regions where the intensity values transition from higher to lower values, while outward edges are tghe regions where the intensity values transition from lower to higher values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416211f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 7/12.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply Laplacian with default Ksize\n",
    "laplacian_default = cv2.Laplacian(image, cv2.CV_64F)\n",
    "\n",
    "# Apply Laplacian with higher ksize (e.g. 11)\n",
    "laplacian_higher = cv2.Laplacian(image, cv2.CV_64F, ksize=7)\n",
    "\n",
    "# Convert the results to unsigned 8-bit for visualization\n",
    "laplacian_default = cv2.convertScaleAbs(laplacian_default)\n",
    "laplacian_higher = cv2.convertScaleAbs(laplacian_higher)\n",
    "\n",
    "#Display the original image and Laplacian results\n",
    "cv2.imshow(\"Original Image\", image)\n",
    "cv2.imshow(\"Laplacian (Default Ksize)\", laplacian_default)\n",
    "cv2.imshow(\"Laplacian (Higher ksize)\", laplacian_higher)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5b434",
   "metadata": {},
   "source": [
    "### Canny Edge Detector\n",
    "The canny edge detector is widely used algorithm for edge detection in image processing. The algorithm was developed in 1986 by John F Canny and has since become a pivotal advancement in the field of computer vision and image processing. The Canny edge detector aims to accuratel identify the boundaries of objects in an image while minimizing noise and false detections. \n",
    "\n",
    "The canny edge detector provides reliable and precise edge detection results by leveraging a set of carefully designed steps.\n",
    "\n",
    "##### Step1:\n",
    "Guassian Smoothing,The image is converted in grayscale and guassian blur is applied on the image to reduce noise. Smoothing the image will allow us to remove some details from the image, since we are not interested in the small details and want to extract the main boundaries in the image.\n",
    "\n",
    "#### Step2:\n",
    "Gradient Magnitude and direction: Gradients in an image represent the rate of change in pixel intensities in an image. Gradient magnitude represents the strength or the magnitude of the gradients while gradient direction talks about the direction or the orientation of the gradients in the image. We will calculate these parameters for each pixel in the image using gradient operators such as the Sobel and Scharr.\n",
    "\n",
    "#### Step 3:\n",
    "Non Max Suppression: Non-Max Suppression in Canny Edge detection is a process used in thinning the edges and thus accurately representing the true edges in an image. The algorithm keeps only the significant edges by preserving only the local maximum responses and thinning out any non-maximum responses.\n",
    "\n",
    "Non-Max Suppression works by iterating over each pixel in an image and comparing its values with the gradient magnitudes of neighbouring pixels in the direction perpendicular to the edge indicated by the gradient direction. Non max suppression works by performing the following steps\n",
    "1. Compute the gradient magnitude and direction of the image using gradient operators such as the Sobel or Scharr operators.\n",
    "2. Iterate over each pixel in the gradient magnitude image. \n",
    "3. Compare the gradient magnitude of the current pixel with its neighbouing pixels in the gradient direction.\n",
    "4. If the current pixel has greater magnitude than its neighbours, it is considered as a candidate for an edge pixel. If the current pixel has a lower magnitude than its neighbours, then the pixel is ignored. \n",
    "5. The value for this pixel is then set to its gradient magnitude value indicating that it is a local minimum response. Pixels that are not considered as local maxima are set to 0.\n",
    "\n",
    "By performing the non maximum supression, the algorithm ensures that only pixels with maximum gradient magnitudes along the edges are retained, while suppressing the weaker responses that do not corresponds to the sharpest edges.\n",
    "\n",
    "#### Step 4:\n",
    "Hysteresis Thresholding: There are still a few regions in the image that are not edges and need to be removed. To do that, we first choose two threshold values, a higher and a lower threshold value and use these to classify pixels into strong, weak or non edges. These threshold values have to be chosen carefully as a large range between these values will keep a lot of false edges while a narrow range might eliminate some real edges from the output.\n",
    "Pixels with thresholds higher than the upper threshold value are classified as strong edges and these pixels are kept. Pixels with lower values than the lower threshold are considered to be non-edges and these pixels are discarded as edges. \n",
    "Any values lying between the upper and lower threshold values are classified as weak edges. If the weak edge pixels are connected to a strong edge, these pixels are kept and marked as edge. Otherwise these values are discarded. This is known as edge tracking by hystersis and helps to extend and connect edges that may have been broken during thresholding. \n",
    "Output: The result is and edge map where edges are represented as white and all non-edge areas are depicted as black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929a08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"Images/Input Images/Chapter 7/12.jpg\", cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6efa9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "#Compute the Gradients using Sobel Operator\n",
    "gradient_x = cv2.Sobel(blurred, cv2.CV_64F, 1, 0, ksize=3)\n",
    "gradient_y = cv2.Sobel(blurred, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "#Compute the magnitude and direction of the gradients\n",
    "gradient_magnitude = np.sqrt(gradient_x**2, gradient_y**2)\n",
    "gradient_direction = np.arctan2(gradient_y, gradient_x)\n",
    "cv2.imwrite('gradient_magnitude.jpg', gradient_magnitude)\n",
    "cv2.imwrite('gradient_direction.jpg', gradient_direction)\n",
    "\n",
    "#Perform non-maximum suppression\n",
    "suppressed = np.copy(gradient_magnitude)\n",
    "for i in range(1, suppressed.shape[0] -1):\n",
    "    for j in range(1, suppressed.shape[1] - 1):\n",
    "        direction = gradient_direction[i, j] * 180./np.pi\n",
    "        if ( 0 <= direction < 22.5) or (157.5 <= direction <= 180):\n",
    "            if suppressed[i, j] <= suppressed[i, j+1] or suppressed[i, j] <= suppressed[i, j - 1]:\n",
    "                suppressed[i, j] = 0\n",
    "        elif (22.5 <= direction < 67.5):\n",
    "            if suppressed[i, j] <= suppressed[i - 1, j + 1] or suppressed[i, j] <= suppressed[i + 1, j - 1]:\n",
    "                suppressed[i, j] = 0\n",
    "        elif (67.5 <= direction < 112.5):\n",
    "            if suppressed[i, j] <= suppressed[i - 1, j] or suppressed[i, j] <= suppressed[i + 1, j]:\n",
    "                suppressed[i, j] = 0\n",
    "        else:\n",
    "            if suppressed[i, j] <= suppressed[i - 1, j - 1] or suppressed[i, j] <= suppressed[i + 1, j + 1]:\n",
    "                suppressed[i, j] = 0\n",
    "                    \n",
    "cv2.imwrite('suppressed.jpg', suppressed)\n",
    "\n",
    "# Perform thresholding to classify pixels as strong or weak edges \n",
    "low_threshold = 30\n",
    "high_threshold = 100\n",
    "edges = np.zeros_like(suppressed)\n",
    "edges[suppressed >= high_threshold] = 255\n",
    "edges[suppressed <= low_threshold] = 0\n",
    "weak_edges = np.logical_and(suppressed > low_threshold, suppressed < high_threshold )\n",
    "\n",
    "# Perform edge tracking by connecting weak edges to strong edges\n",
    "strong_edges_i , strong_edges_j = np.where(edges == 255)\n",
    "for i, j in zip(strong_edges_i, strong_edges_j):\n",
    "    if np.any(weak_edges[i - 1:i + 2, j - 1:j + 2]):\n",
    "        edges[i - 1:i + 2, j - 1: j + 2] = 255\n",
    "cv2.imwrite('edges.jpg', edges)\n",
    "\n",
    "cv2.imshow('Canny Edges', edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd4aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "canny_edges =cv2.Canny(image, threshold1=30, threshold2=100)\n",
    "cv2.imshow('Edges', canny_edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f54590",
   "metadata": {},
   "source": [
    "## Introduction to Contours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ca9521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Contours: (array([[[221, 400]],\n",
      "\n",
      "       [[221, 401]],\n",
      "\n",
      "       [[220, 402]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[222, 403]],\n",
      "\n",
      "       [[222, 402]],\n",
      "\n",
      "       [[221, 401]]], dtype=int32), array([[[161, 539]],\n",
      "\n",
      "       [[162, 538]],\n",
      "\n",
      "       [[163, 539]],\n",
      "\n",
      "       [[162, 540]]], dtype=int32), array([[[550, 225]],\n",
      "\n",
      "       [[549, 226]],\n",
      "\n",
      "       [[532, 226]],\n",
      "\n",
      "       [[531, 227]],\n",
      "\n",
      "       [[524, 227]],\n",
      "\n",
      "       [[523, 228]],\n",
      "\n",
      "       [[517, 228]],\n",
      "\n",
      "       [[516, 229]],\n",
      "\n",
      "       [[513, 229]],\n",
      "\n",
      "       [[512, 230]],\n",
      "\n",
      "       [[508, 230]],\n",
      "\n",
      "       [[507, 231]],\n",
      "\n",
      "       [[505, 231]],\n",
      "\n",
      "       [[504, 232]],\n",
      "\n",
      "       [[500, 232]],\n",
      "\n",
      "       [[499, 233]],\n",
      "\n",
      "       [[498, 233]],\n",
      "\n",
      "       [[497, 234]],\n",
      "\n",
      "       [[494, 234]],\n",
      "\n",
      "       [[493, 235]],\n",
      "\n",
      "       [[492, 235]],\n",
      "\n",
      "       [[491, 236]],\n",
      "\n",
      "       [[488, 236]],\n",
      "\n",
      "       [[486, 238]],\n",
      "\n",
      "       [[484, 238]],\n",
      "\n",
      "       [[483, 239]],\n",
      "\n",
      "       [[482, 239]],\n",
      "\n",
      "       [[481, 240]],\n",
      "\n",
      "       [[480, 240]],\n",
      "\n",
      "       [[479, 241]],\n",
      "\n",
      "       [[477, 241]],\n",
      "\n",
      "       [[476, 242]],\n",
      "\n",
      "       [[475, 242]],\n",
      "\n",
      "       [[474, 243]],\n",
      "\n",
      "       [[473, 243]],\n",
      "\n",
      "       [[472, 244]],\n",
      "\n",
      "       [[471, 244]],\n",
      "\n",
      "       [[470, 245]],\n",
      "\n",
      "       [[469, 245]],\n",
      "\n",
      "       [[468, 246]],\n",
      "\n",
      "       [[467, 246]],\n",
      "\n",
      "       [[465, 248]],\n",
      "\n",
      "       [[463, 248]],\n",
      "\n",
      "       [[461, 250]],\n",
      "\n",
      "       [[460, 250]],\n",
      "\n",
      "       [[458, 252]],\n",
      "\n",
      "       [[457, 252]],\n",
      "\n",
      "       [[455, 254]],\n",
      "\n",
      "       [[454, 254]],\n",
      "\n",
      "       [[452, 256]],\n",
      "\n",
      "       [[451, 256]],\n",
      "\n",
      "       [[449, 258]],\n",
      "\n",
      "       [[448, 258]],\n",
      "\n",
      "       [[446, 260]],\n",
      "\n",
      "       [[445, 260]],\n",
      "\n",
      "       [[441, 264]],\n",
      "\n",
      "       [[440, 264]],\n",
      "\n",
      "       [[436, 268]],\n",
      "\n",
      "       [[435, 268]],\n",
      "\n",
      "       [[418, 285]],\n",
      "\n",
      "       [[418, 286]],\n",
      "\n",
      "       [[414, 290]],\n",
      "\n",
      "       [[414, 291]],\n",
      "\n",
      "       [[410, 295]],\n",
      "\n",
      "       [[410, 296]],\n",
      "\n",
      "       [[408, 298]],\n",
      "\n",
      "       [[408, 299]],\n",
      "\n",
      "       [[406, 301]],\n",
      "\n",
      "       [[406, 302]],\n",
      "\n",
      "       [[404, 304]],\n",
      "\n",
      "       [[404, 305]],\n",
      "\n",
      "       [[402, 307]],\n",
      "\n",
      "       [[402, 308]],\n",
      "\n",
      "       [[400, 310]],\n",
      "\n",
      "       [[400, 311]],\n",
      "\n",
      "       [[399, 312]],\n",
      "\n",
      "       [[399, 313]],\n",
      "\n",
      "       [[398, 314]],\n",
      "\n",
      "       [[398, 315]],\n",
      "\n",
      "       [[396, 317]],\n",
      "\n",
      "       [[396, 318]],\n",
      "\n",
      "       [[395, 319]],\n",
      "\n",
      "       [[395, 320]],\n",
      "\n",
      "       [[394, 321]],\n",
      "\n",
      "       [[394, 322]],\n",
      "\n",
      "       [[393, 323]],\n",
      "\n",
      "       [[393, 324]],\n",
      "\n",
      "       [[392, 325]],\n",
      "\n",
      "       [[392, 326]],\n",
      "\n",
      "       [[391, 327]],\n",
      "\n",
      "       [[391, 328]],\n",
      "\n",
      "       [[390, 329]],\n",
      "\n",
      "       [[390, 331]],\n",
      "\n",
      "       [[388, 333]],\n",
      "\n",
      "       [[388, 336]],\n",
      "\n",
      "       [[387, 337]],\n",
      "\n",
      "       [[387, 338]],\n",
      "\n",
      "       [[386, 339]],\n",
      "\n",
      "       [[386, 341]],\n",
      "\n",
      "       [[385, 342]],\n",
      "\n",
      "       [[385, 343]],\n",
      "\n",
      "       [[384, 344]],\n",
      "\n",
      "       [[384, 347]],\n",
      "\n",
      "       [[383, 348]],\n",
      "\n",
      "       [[383, 349]],\n",
      "\n",
      "       [[382, 350]],\n",
      "\n",
      "       [[382, 354]],\n",
      "\n",
      "       [[381, 355]],\n",
      "\n",
      "       [[381, 357]],\n",
      "\n",
      "       [[380, 358]],\n",
      "\n",
      "       [[380, 362]],\n",
      "\n",
      "       [[379, 363]],\n",
      "\n",
      "       [[379, 367]],\n",
      "\n",
      "       [[378, 368]],\n",
      "\n",
      "       [[378, 373]],\n",
      "\n",
      "       [[377, 374]],\n",
      "\n",
      "       [[377, 381]],\n",
      "\n",
      "       [[376, 382]],\n",
      "\n",
      "       [[376, 399]],\n",
      "\n",
      "       [[375, 400]],\n",
      "\n",
      "       [[376, 401]],\n",
      "\n",
      "       [[376, 419]],\n",
      "\n",
      "       [[377, 420]],\n",
      "\n",
      "       [[377, 426]],\n",
      "\n",
      "       [[378, 427]],\n",
      "\n",
      "       [[378, 433]],\n",
      "\n",
      "       [[379, 434]],\n",
      "\n",
      "       [[379, 437]],\n",
      "\n",
      "       [[380, 438]],\n",
      "\n",
      "       [[380, 442]],\n",
      "\n",
      "       [[381, 443]],\n",
      "\n",
      "       [[381, 445]],\n",
      "\n",
      "       [[382, 446]],\n",
      "\n",
      "       [[382, 450]],\n",
      "\n",
      "       [[383, 451]],\n",
      "\n",
      "       [[383, 452]],\n",
      "\n",
      "       [[384, 453]],\n",
      "\n",
      "       [[384, 455]],\n",
      "\n",
      "       [[385, 456]],\n",
      "\n",
      "       [[385, 458]],\n",
      "\n",
      "       [[386, 459]],\n",
      "\n",
      "       [[386, 462]],\n",
      "\n",
      "       [[388, 464]],\n",
      "\n",
      "       [[388, 466]],\n",
      "\n",
      "       [[389, 467]],\n",
      "\n",
      "       [[389, 468]],\n",
      "\n",
      "       [[390, 469]],\n",
      "\n",
      "       [[390, 471]],\n",
      "\n",
      "       [[391, 472]],\n",
      "\n",
      "       [[391, 473]],\n",
      "\n",
      "       [[392, 474]],\n",
      "\n",
      "       [[392, 475]],\n",
      "\n",
      "       [[393, 476]],\n",
      "\n",
      "       [[393, 477]],\n",
      "\n",
      "       [[394, 478]],\n",
      "\n",
      "       [[394, 479]],\n",
      "\n",
      "       [[395, 480]],\n",
      "\n",
      "       [[395, 481]],\n",
      "\n",
      "       [[396, 482]],\n",
      "\n",
      "       [[396, 483]],\n",
      "\n",
      "       [[398, 485]],\n",
      "\n",
      "       [[398, 486]],\n",
      "\n",
      "       [[399, 487]],\n",
      "\n",
      "       [[399, 488]],\n",
      "\n",
      "       [[400, 489]],\n",
      "\n",
      "       [[400, 490]],\n",
      "\n",
      "       [[402, 492]],\n",
      "\n",
      "       [[402, 493]],\n",
      "\n",
      "       [[404, 495]],\n",
      "\n",
      "       [[404, 496]],\n",
      "\n",
      "       [[406, 498]],\n",
      "\n",
      "       [[406, 499]],\n",
      "\n",
      "       [[408, 501]],\n",
      "\n",
      "       [[408, 502]],\n",
      "\n",
      "       [[410, 504]],\n",
      "\n",
      "       [[410, 505]],\n",
      "\n",
      "       [[414, 509]],\n",
      "\n",
      "       [[414, 510]],\n",
      "\n",
      "       [[419, 515]],\n",
      "\n",
      "       [[419, 516]],\n",
      "\n",
      "       [[434, 531]],\n",
      "\n",
      "       [[435, 531]],\n",
      "\n",
      "       [[441, 537]],\n",
      "\n",
      "       [[442, 537]],\n",
      "\n",
      "       [[446, 541]],\n",
      "\n",
      "       [[447, 541]],\n",
      "\n",
      "       [[448, 542]],\n",
      "\n",
      "       [[449, 542]],\n",
      "\n",
      "       [[451, 544]],\n",
      "\n",
      "       [[452, 544]],\n",
      "\n",
      "       [[455, 547]],\n",
      "\n",
      "       [[456, 547]],\n",
      "\n",
      "       [[457, 548]],\n",
      "\n",
      "       [[458, 548]],\n",
      "\n",
      "       [[460, 550]],\n",
      "\n",
      "       [[461, 550]],\n",
      "\n",
      "       [[462, 551]],\n",
      "\n",
      "       [[463, 551]],\n",
      "\n",
      "       [[464, 552]],\n",
      "\n",
      "       [[465, 552]],\n",
      "\n",
      "       [[468, 555]],\n",
      "\n",
      "       [[470, 555]],\n",
      "\n",
      "       [[471, 556]],\n",
      "\n",
      "       [[472, 556]],\n",
      "\n",
      "       [[473, 557]],\n",
      "\n",
      "       [[474, 557]],\n",
      "\n",
      "       [[475, 558]],\n",
      "\n",
      "       [[476, 558]],\n",
      "\n",
      "       [[477, 559]],\n",
      "\n",
      "       [[479, 559]],\n",
      "\n",
      "       [[480, 560]],\n",
      "\n",
      "       [[481, 560]],\n",
      "\n",
      "       [[482, 561]],\n",
      "\n",
      "       [[483, 561]],\n",
      "\n",
      "       [[484, 562]],\n",
      "\n",
      "       [[485, 562]],\n",
      "\n",
      "       [[486, 563]],\n",
      "\n",
      "       [[488, 563]],\n",
      "\n",
      "       [[489, 564]],\n",
      "\n",
      "       [[491, 564]],\n",
      "\n",
      "       [[492, 565]],\n",
      "\n",
      "       [[494, 565]],\n",
      "\n",
      "       [[495, 566]],\n",
      "\n",
      "       [[497, 566]],\n",
      "\n",
      "       [[498, 567]],\n",
      "\n",
      "       [[500, 567]],\n",
      "\n",
      "       [[501, 568]],\n",
      "\n",
      "       [[503, 568]],\n",
      "\n",
      "       [[504, 569]],\n",
      "\n",
      "       [[508, 569]],\n",
      "\n",
      "       [[509, 570]],\n",
      "\n",
      "       [[511, 570]],\n",
      "\n",
      "       [[512, 571]],\n",
      "\n",
      "       [[517, 571]],\n",
      "\n",
      "       [[518, 572]],\n",
      "\n",
      "       [[523, 572]],\n",
      "\n",
      "       [[524, 573]],\n",
      "\n",
      "       [[531, 573]],\n",
      "\n",
      "       [[532, 574]],\n",
      "\n",
      "       [[549, 574]],\n",
      "\n",
      "       [[550, 575]],\n",
      "\n",
      "       [[551, 575]],\n",
      "\n",
      "       [[552, 574]],\n",
      "\n",
      "       [[568, 574]],\n",
      "\n",
      "       [[569, 573]],\n",
      "\n",
      "       [[577, 573]],\n",
      "\n",
      "       [[578, 572]],\n",
      "\n",
      "       [[582, 572]],\n",
      "\n",
      "       [[583, 571]],\n",
      "\n",
      "       [[587, 571]],\n",
      "\n",
      "       [[588, 570]],\n",
      "\n",
      "       [[591, 570]],\n",
      "\n",
      "       [[592, 569]],\n",
      "\n",
      "       [[595, 569]],\n",
      "\n",
      "       [[596, 568]],\n",
      "\n",
      "       [[599, 568]],\n",
      "\n",
      "       [[600, 567]],\n",
      "\n",
      "       [[603, 567]],\n",
      "\n",
      "       [[604, 566]],\n",
      "\n",
      "       [[605, 566]],\n",
      "\n",
      "       [[606, 565]],\n",
      "\n",
      "       [[609, 565]],\n",
      "\n",
      "       [[610, 564]],\n",
      "\n",
      "       [[611, 564]],\n",
      "\n",
      "       [[612, 563]],\n",
      "\n",
      "       [[613, 563]],\n",
      "\n",
      "       [[614, 562]],\n",
      "\n",
      "       [[616, 562]],\n",
      "\n",
      "       [[617, 561]],\n",
      "\n",
      "       [[619, 561]],\n",
      "\n",
      "       [[621, 559]],\n",
      "\n",
      "       [[623, 559]],\n",
      "\n",
      "       [[624, 558]],\n",
      "\n",
      "       [[625, 558]],\n",
      "\n",
      "       [[626, 557]],\n",
      "\n",
      "       [[627, 557]],\n",
      "\n",
      "       [[628, 556]],\n",
      "\n",
      "       [[629, 556]],\n",
      "\n",
      "       [[630, 555]],\n",
      "\n",
      "       [[632, 555]],\n",
      "\n",
      "       [[635, 552]],\n",
      "\n",
      "       [[636, 552]],\n",
      "\n",
      "       [[637, 551]],\n",
      "\n",
      "       [[638, 551]],\n",
      "\n",
      "       [[639, 550]],\n",
      "\n",
      "       [[640, 550]],\n",
      "\n",
      "       [[641, 549]],\n",
      "\n",
      "       [[642, 549]],\n",
      "\n",
      "       [[645, 546]],\n",
      "\n",
      "       [[646, 546]],\n",
      "\n",
      "       [[647, 545]],\n",
      "\n",
      "       [[648, 545]],\n",
      "\n",
      "       [[651, 542]],\n",
      "\n",
      "       [[652, 542]],\n",
      "\n",
      "       [[653, 541]],\n",
      "\n",
      "       [[654, 541]],\n",
      "\n",
      "       [[659, 536]],\n",
      "\n",
      "       [[660, 536]],\n",
      "\n",
      "       [[665, 531]],\n",
      "\n",
      "       [[666, 531]],\n",
      "\n",
      "       [[681, 516]],\n",
      "\n",
      "       [[681, 515]],\n",
      "\n",
      "       [[686, 510]],\n",
      "\n",
      "       [[686, 509]],\n",
      "\n",
      "       [[691, 504]],\n",
      "\n",
      "       [[691, 503]],\n",
      "\n",
      "       [[692, 502]],\n",
      "\n",
      "       [[692, 501]],\n",
      "\n",
      "       [[695, 498]],\n",
      "\n",
      "       [[695, 497]],\n",
      "\n",
      "       [[697, 495]],\n",
      "\n",
      "       [[697, 494]],\n",
      "\n",
      "       [[699, 492]],\n",
      "\n",
      "       [[699, 491]],\n",
      "\n",
      "       [[700, 490]],\n",
      "\n",
      "       [[700, 489]],\n",
      "\n",
      "       [[701, 488]],\n",
      "\n",
      "       [[701, 487]],\n",
      "\n",
      "       [[702, 486]],\n",
      "\n",
      "       [[702, 485]],\n",
      "\n",
      "       [[704, 483]],\n",
      "\n",
      "       [[704, 482]],\n",
      "\n",
      "       [[705, 481]],\n",
      "\n",
      "       [[705, 480]],\n",
      "\n",
      "       [[707, 478]],\n",
      "\n",
      "       [[707, 476]],\n",
      "\n",
      "       [[709, 474]],\n",
      "\n",
      "       [[709, 471]],\n",
      "\n",
      "       [[710, 470]],\n",
      "\n",
      "       [[710, 469]],\n",
      "\n",
      "       [[711, 468]],\n",
      "\n",
      "       [[711, 467]],\n",
      "\n",
      "       [[712, 466]],\n",
      "\n",
      "       [[712, 464]],\n",
      "\n",
      "       [[713, 463]],\n",
      "\n",
      "       [[713, 462]],\n",
      "\n",
      "       [[714, 461]],\n",
      "\n",
      "       [[714, 460]],\n",
      "\n",
      "       [[715, 459]],\n",
      "\n",
      "       [[715, 456]],\n",
      "\n",
      "       [[716, 455]],\n",
      "\n",
      "       [[716, 454]],\n",
      "\n",
      "       [[717, 453]],\n",
      "\n",
      "       [[717, 450]],\n",
      "\n",
      "       [[718, 449]],\n",
      "\n",
      "       [[718, 446]],\n",
      "\n",
      "       [[719, 445]],\n",
      "\n",
      "       [[719, 442]],\n",
      "\n",
      "       [[720, 441]],\n",
      "\n",
      "       [[720, 438]],\n",
      "\n",
      "       [[721, 437]],\n",
      "\n",
      "       [[721, 433]],\n",
      "\n",
      "       [[722, 432]],\n",
      "\n",
      "       [[722, 428]],\n",
      "\n",
      "       [[723, 427]],\n",
      "\n",
      "       [[723, 419]],\n",
      "\n",
      "       [[724, 418]],\n",
      "\n",
      "       [[724, 402]],\n",
      "\n",
      "       [[725, 401]],\n",
      "\n",
      "       [[725, 400]],\n",
      "\n",
      "       [[724, 399]],\n",
      "\n",
      "       [[724, 382]],\n",
      "\n",
      "       [[723, 381]],\n",
      "\n",
      "       [[723, 374]],\n",
      "\n",
      "       [[722, 373]],\n",
      "\n",
      "       [[722, 368]],\n",
      "\n",
      "       [[721, 367]],\n",
      "\n",
      "       [[721, 362]],\n",
      "\n",
      "       [[720, 361]],\n",
      "\n",
      "       [[720, 359]],\n",
      "\n",
      "       [[719, 358]],\n",
      "\n",
      "       [[719, 354]],\n",
      "\n",
      "       [[718, 353]],\n",
      "\n",
      "       [[718, 351]],\n",
      "\n",
      "       [[717, 350]],\n",
      "\n",
      "       [[717, 348]],\n",
      "\n",
      "       [[716, 347]],\n",
      "\n",
      "       [[716, 345]],\n",
      "\n",
      "       [[715, 344]],\n",
      "\n",
      "       [[715, 341]],\n",
      "\n",
      "       [[714, 340]],\n",
      "\n",
      "       [[714, 339]],\n",
      "\n",
      "       [[713, 338]],\n",
      "\n",
      "       [[713, 336]],\n",
      "\n",
      "       [[712, 335]],\n",
      "\n",
      "       [[712, 334]],\n",
      "\n",
      "       [[711, 333]],\n",
      "\n",
      "       [[711, 332]],\n",
      "\n",
      "       [[710, 331]],\n",
      "\n",
      "       [[710, 330]],\n",
      "\n",
      "       [[709, 329]],\n",
      "\n",
      "       [[709, 327]],\n",
      "\n",
      "       [[708, 326]],\n",
      "\n",
      "       [[708, 325]],\n",
      "\n",
      "       [[707, 324]],\n",
      "\n",
      "       [[707, 323]],\n",
      "\n",
      "       [[706, 322]],\n",
      "\n",
      "       [[706, 321]],\n",
      "\n",
      "       [[705, 320]],\n",
      "\n",
      "       [[705, 318]],\n",
      "\n",
      "       [[702, 315]],\n",
      "\n",
      "       [[702, 314]],\n",
      "\n",
      "       [[701, 313]],\n",
      "\n",
      "       [[701, 311]],\n",
      "\n",
      "       [[698, 308]],\n",
      "\n",
      "       [[698, 307]],\n",
      "\n",
      "       [[697, 306]],\n",
      "\n",
      "       [[697, 305]],\n",
      "\n",
      "       [[694, 302]],\n",
      "\n",
      "       [[694, 301]],\n",
      "\n",
      "       [[693, 300]],\n",
      "\n",
      "       [[693, 299]],\n",
      "\n",
      "       [[690, 296]],\n",
      "\n",
      "       [[690, 295]],\n",
      "\n",
      "       [[687, 292]],\n",
      "\n",
      "       [[687, 291]],\n",
      "\n",
      "       [[681, 285]],\n",
      "\n",
      "       [[681, 284]],\n",
      "\n",
      "       [[665, 268]],\n",
      "\n",
      "       [[664, 268]],\n",
      "\n",
      "       [[660, 264]],\n",
      "\n",
      "       [[659, 264]],\n",
      "\n",
      "       [[655, 260]],\n",
      "\n",
      "       [[654, 260]],\n",
      "\n",
      "       [[652, 258]],\n",
      "\n",
      "       [[651, 258]],\n",
      "\n",
      "       [[649, 256]],\n",
      "\n",
      "       [[648, 256]],\n",
      "\n",
      "       [[646, 254]],\n",
      "\n",
      "       [[645, 254]],\n",
      "\n",
      "       [[643, 252]],\n",
      "\n",
      "       [[642, 252]],\n",
      "\n",
      "       [[640, 250]],\n",
      "\n",
      "       [[639, 250]],\n",
      "\n",
      "       [[638, 249]],\n",
      "\n",
      "       [[637, 249]],\n",
      "\n",
      "       [[636, 248]],\n",
      "\n",
      "       [[635, 248]],\n",
      "\n",
      "       [[633, 246]],\n",
      "\n",
      "       [[632, 246]],\n",
      "\n",
      "       [[630, 244]],\n",
      "\n",
      "       [[628, 244]],\n",
      "\n",
      "       [[626, 242]],\n",
      "\n",
      "       [[624, 242]],\n",
      "\n",
      "       [[623, 241]],\n",
      "\n",
      "       [[622, 241]],\n",
      "\n",
      "       [[621, 240]],\n",
      "\n",
      "       [[619, 240]],\n",
      "\n",
      "       [[618, 239]],\n",
      "\n",
      "       [[617, 239]],\n",
      "\n",
      "       [[616, 238]],\n",
      "\n",
      "       [[614, 238]],\n",
      "\n",
      "       [[613, 237]],\n",
      "\n",
      "       [[612, 237]],\n",
      "\n",
      "       [[611, 236]],\n",
      "\n",
      "       [[609, 236]],\n",
      "\n",
      "       [[608, 235]],\n",
      "\n",
      "       [[607, 235]],\n",
      "\n",
      "       [[606, 234]],\n",
      "\n",
      "       [[603, 234]],\n",
      "\n",
      "       [[602, 233]],\n",
      "\n",
      "       [[600, 233]],\n",
      "\n",
      "       [[599, 232]],\n",
      "\n",
      "       [[596, 232]],\n",
      "\n",
      "       [[595, 231]],\n",
      "\n",
      "       [[593, 231]],\n",
      "\n",
      "       [[592, 230]],\n",
      "\n",
      "       [[588, 230]],\n",
      "\n",
      "       [[587, 229]],\n",
      "\n",
      "       [[584, 229]],\n",
      "\n",
      "       [[583, 228]],\n",
      "\n",
      "       [[577, 228]],\n",
      "\n",
      "       [[576, 227]],\n",
      "\n",
      "       [[570, 227]],\n",
      "\n",
      "       [[569, 226]],\n",
      "\n",
      "       [[551, 226]]], dtype=int32), array([[[ 50,  50]],\n",
      "\n",
      "       [[ 50, 303]],\n",
      "\n",
      "       [[302, 303]],\n",
      "\n",
      "       [[302,  50]]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('Images/Input Images/Chapter 7/objects.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow(\"Gray Image\", gray)\n",
    "\n",
    "#Applly thresholding to create a binary image\n",
    "_, thresh = cv2.threshold(gray, 25, 255, cv2.THRESH_BINARY)\n",
    "print(\"Threshold: {}\".format(thresh))\n",
    "\n",
    "#Find contours\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(\"Contours: {}\".format(contours))\n",
    "\n",
    "#Create a copy of the original image\n",
    "contour_image = image.copy()\n",
    "\n",
    "#Draw contours on the copy of the original image\n",
    "cv2.drawContours(contour_image,contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Contours', contour_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21331eaf",
   "metadata": {},
   "source": [
    "## Contour Moments\n",
    "Contour Moments are statistical measures that describe various characteristics of a contour or shape. These moments provide valuable information about the spatial distribution, size, shape and orientation of objects in an image. By analyzing contour moments, we can extract important features and properties of objects, which can be used for various tasks such as object recognition, shape matching and classification.\n",
    "\n",
    "There are four different types of contour momemts that capture different aspects of the shape.\n",
    "The most commonly used momemt are:\n",
    "##### Area:\n",
    "Represent the total area enclosed by the contour\n",
    "\n",
    "##### Centroid: \n",
    "Indicates the center of mass or the average position of the contour\n",
    "\n",
    "##### Moment of Inertia:\n",
    "Provide information about the objects resistance to rotation around centroid.\n",
    "\n",
    "##### Orientation:\n",
    "Describes the angle at which the major axis of the object is aligned\n",
    "\n",
    "##### Hu Moments: \n",
    "A Set of seven invariant moments that are robust to translation, rotation, and scale changes. \n",
    "\n",
    "Contour moments can be calculated using mathematical formulas based on the coordinates. Also, OpenCV provides funtions to compute moments directly from the contours or the binary image containing the contour.\n",
    "\n",
    "\n",
    "### Centroid/Center of Mass\n",
    "The centroid or the Center of Mass property refers to the geometric center point of the contour shape. It is basically the average position of all the contours. \n",
    "\n",
    "In OpenCV, the centroid can be calculated using Moments of a contour. The moments refer to statistical properties of a contour shape. The cv2.moments() function in OpenCV can be used to calculate the moments of a contour, and the centroid can be extracted from there. \n",
    "\n",
    "\n",
    "The Centroid of a contour can be calculated using the following formula:\n",
    "CentroidX = m10/m00\n",
    "CentroidY = m01/m00\n",
    "\n",
    "Centroid - (CentroidX, CentroidY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb3aaaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: 38313\n",
      "Perimeter: 958\n",
      "Centroid (x, y): 221 598\n",
      "Area: 95860\n",
      "Perimeter: 1171\n",
      "Centroid (x, y): 550 400\n",
      "Area: 63756\n",
      "Perimeter: 1010\n",
      "Centroid (x, y): 176 176\n"
     ]
    }
   ],
   "source": [
    "image =cv2.imread('Images/Input Images/Chapter 7/objects.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Apply thresholding to obtain a binary image\n",
    "_, binary = cv2.threshold(gray, 22, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#Find contours in binary image\n",
    "contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#Iterate over the contours\n",
    "for contour in contours:\n",
    "    #Calculate the Aread of the contour\n",
    "    area = int(cv2.contourArea(contour))\n",
    "    \n",
    "    #Calculate the perimeter of the contour\n",
    "    perimeter = int(cv2.arcLength(contour, True))\n",
    "    \n",
    "    #Calculate the centroid of the contour\n",
    "    M = cv2.moments(contour)\n",
    "    centroid_x = int(M['m10'] / M['m00'])\n",
    "    centroid_y = int(M['m01'] / M['m00'])\n",
    "    \n",
    "    print(\"Area:\", area)\n",
    "    print(\"Perimeter:\", perimeter)\n",
    "    print(\"Centroid (x, y):\", centroid_x, centroid_y)\n",
    "    \n",
    "    #Draw the contour, centroid and text on the image\n",
    "    cv2.drawContours(image, [contour], 0, (0, 255, 0), 2)\n",
    "    cv2.circle(image, (centroid_x, centroid_y), 5, (0, 0, 0), -1)\n",
    "    cv2.putText(image, f\"Area: {area}\", (centroid_x-50, centroid_y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    cv2.putText(image, f\"Perimeter: {perimeter}\", (centroid_x - 80, centroid_y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "cv2.imshow(\"Object Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612535b",
   "metadata": {},
   "source": [
    "### Bounding Rectangle\n",
    "Abounding rectangle refers to the smallest rectangle that can completely enclose an object in an image. The bounding rectangle is typically represented as the top-left coordinates as (x, y) followed by the width and height of the rectangle.\n",
    "\n",
    "The bounding rectangle can be used for number of OpenCV tasks as it provides a simple way to describe the locatioon and extent of an object within an image which can be further utilized for analysis or further processing. \n",
    "\n",
    "In OpenCV, the bounding rectangle of a contour can be obtained using the cv2.boundingRect() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300c1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 7/rectangles.jpg')\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply thresholding to create a binary image\n",
    "_, thresh = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY )\n",
    "\n",
    "# Find Contours\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n",
    "\n",
    "# Create a copy of the original image\n",
    "bounding_rect_image = image.copy()\n",
    "\n",
    "for contour in contours:\n",
    "    # Get the bounding rectangel for the contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Draw a rectangle around the object.\n",
    "    cv2.rectangle(bounding_rect_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "cv2.imshow('Original Image', gray)\n",
    "cv2.imshow('Bounding Rectangles', bounding_rect_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60ddc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((x, y), (width, height), angle): ((124.50000762939453, 499.4999694824219), (197.1002960205078, 81.59115600585938), 19.98310661315918)\n",
      "Box: [[ 17 504]\n",
      " [ 45 427]\n",
      " [231 494]\n",
      " [203 571]]\n",
      "((x, y), (width, height), angle): ((349.4855651855469, 299.4916687011719), (146.4519805908203, 211.18270874023438), 29.981639862060547)\n",
      "Box: [[233 354]\n",
      " [338 171]\n",
      " [465 244]\n",
      " [360 427]]\n",
      "((x, y), (width, height), angle): ((150.0, 149.5), (175.0, 100.0), 90.0)\n",
      "Box: [[100  62]\n",
      " [200  62]\n",
      " [200 237]\n",
      " [100 237]]\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 7/tects.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Apply thresholding to create a binary image\n",
    "_, thresh = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY )\n",
    "\n",
    "#Find contours\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n",
    "\n",
    "#Create a copy of the original image\n",
    "bounding_rect_image = image.copy()\n",
    "\n",
    "colors= [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
    "\n",
    "#Loop through the contours\n",
    "for index, contour in enumerate(contours):\n",
    "    # Get the bounding rectangle for the contour\n",
    "    rect = cv2.minAreaRect(contour)\n",
    "    \n",
    "    print(\"((x, y), (width, height), angle): {}\".format(rect))\n",
    "    \n",
    "    #Draw a rectangle around the object\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    print(\"Box: {}\".format(box))\n",
    "    \n",
    "    # Draw the minimum bounding rectangel on the image\n",
    "    cv2.drawContours(image, [box], 0, colors[index], 2)\n",
    "    \n",
    "cv2.imshow('Original Image', gray)\n",
    "cv2.imshow('Bounding Rectangles', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e77bf",
   "metadata": {},
   "source": [
    "### Aspect Ratio\n",
    "The Aspect Ration is a property of contours that describes the propotional relationship between the width and height of the bounding rectangle that encloses the contour. The aspect ratio is calculated as the ratio of the width to the height fo the bounding rectangle:\n",
    "##### Aspect Ratio = Width / Height\n",
    "\n",
    "A contour with an aspect ration close to 1 indicates a shape that is nearly square or circular, while a contour with aspect ratio significantly different from 1 suggests more elongated or stretched shape.\n",
    "\n",
    "### Extent\n",
    "Extent represents how much space or area a shape covers within a given boundary. It tells us how well the shape fills up the available space inside a rectangle that encloses it .\n",
    "Extent is calculated as the ratio between the area of the contour and the area of the bounding rectangle that encloses it.\n",
    "\n",
    "##### Extent = (Contour Area)/(Bounding Rectangle Area)\n",
    "\n",
    "Extent can be used in object recognition tasks particularly where we need to distinguish between objects that may have similar shapes but differ in terms of their area distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2203f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "image =cv2.imread('Images/Input Images/Chapter 7/image_rect.jpg')\n",
    "image_copy = image.copy()\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, binary = cv2.threshold(gray, 20, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Find Contours in Binary Image\n",
    "contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n",
    "\n",
    "for contour in contours:\n",
    "    # Calculate aspect ratio using minimum area rectangle\n",
    "    rect = cv2.minAreaRect(contour)\n",
    "    width, height = rect[1]\n",
    "    aspect_ratio = int(width)/int(height)\n",
    "    \n",
    "    #Calculate the Extent\n",
    "    area = cv2.contourArea(contour)\n",
    "    bounding_area = width*height\n",
    "    extent = area / bounding_area\n",
    "    \n",
    "    #Classify object based on aspect ratio\n",
    "    if aspect_ratio >= 1.5 or aspect_ratio <= 0.95:\n",
    "        # Rectangle\n",
    "        cv2.drawContours(image, [contour], 0, (255, 0, 255), 2)\n",
    "        cv2.putText(image, \"Rectangle\", (int(rect[0][0]), int(rect[0][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        \n",
    "    else:\n",
    "        #Circle\n",
    "        cv2.drawContours(image, [contour], 0, (0, 255, 255), 2)\n",
    "        cv2.putText(image, \"Circle\", (int(rect[0][0]), int(rect[0][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        \n",
    "    #Classify object based on extent\n",
    "    if extent <= 1.05 and extent >= 0.95:\n",
    "        #Rectangle\n",
    "        print(1)\n",
    "        cv2.drawContours(image_copy, [contour], 0, (255, 0, 255), 2)\n",
    "        cv2.putText(image_copy, 'Rectangle', (int(rect[0][0]), int(rect[0][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    else:\n",
    "        #Circle\n",
    "        print(2)\n",
    "        cv2.drawContours(image_copy, [contour], 0, (255, 255, 255), 2)\n",
    "        cv2.putText(image_copy, 'Circle', (int(rect[0][0]), int(rect[0][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        \n",
    "cv2.imshow('Aspect Ratio Classification', image)\n",
    "cv2.imshow('Extent Classification', image_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8121fa",
   "metadata": {},
   "source": [
    "## Convex Hull\n",
    "Convex Hull refers to the smallest convex polygon or shape that completely encloses a given set of points. In simples words the convex hull is a boundary that wraps around all the points, forming the smallest possible shape with straight edges ensuring that all the points are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6470da5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image = cv2.imread('Images/Input Images/Chapter 7/box_star.jpg')\n",
    "image_copy = image.copy()\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    ", \n",
    "_, threshold = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Find contours in the binary image\n",
    "contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE )\n",
    "\n",
    "for contour in contours:\n",
    "    #Apply convex Hull on the contour\n",
    "    hull = cv2.convexHull(contour)\n",
    "    \n",
    "    #Reshape the hull points on the contour\n",
    "    hull_points = hull.reshape((-1, 1, 2))\n",
    "    \n",
    "    #Draw convex Hull lines on the image\n",
    "    cv2.polylines(image, [hull_points], True, (0, 255, 0), 2)\n",
    "    cv2.drawContours(image_copy, [contour], 0, (255, 0, 255), 2)\n",
    "    \n",
    "cv2.imwrite(\"Convex.jpg\", image)\n",
    "cv2.imwrite(\"AllContours.jpg\", image_copy)\n",
    "\n",
    "plt.title('Convex Hull')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e9d75",
   "metadata": {},
   "source": [
    "### Solidity\n",
    "Solidity is a measure of how closely a shape matches its convex hull, which is the smallest possible convex shape that completely encloses the object. In layman terms, solidity refers to how solid or filled a shape appears\n",
    "If the solidity value is closer to 1, It means that the shape is mostly filled and doesn't have many holes or concave parts. On the other hand, if the solidity value is closer to 0, it means that the shape has many holes or concave regions, making it less solid.Solidity is calculated by dividing the contour area by the area of its convex hull representing the propotion of area covered by convex hull:\n",
    "##### Solidity = (Contour Area)/(Convex Hull Area)\n",
    "\n",
    "### Contour Approximation.\n",
    "Contour approximation is a technique used to simplify representation of contours of objects in an image. Contour approximation aims to simplify the contours by reducing the number of points needed to describe them. It does that by smoothing out the contours and removing the unneccessary details thus making it simpler to describe them. The process aims to keep the original shape and important features of the contours while removing any small irregularities from the contours.\n",
    "\n",
    "By removing the number of points required for representing contours, contour approximation can be significantly reduce the processing time and resources required for the process. Removal of small irregularities from the contours results in smoother representation of an object's boundary.\n",
    "\n",
    "Steps in Contour approximation.\n",
    "##### Step1: \n",
    "Extract Contours: First we need to extract the contours of objects in an image. We need to apply pre-processing steps such as thresholding first followed by a contour detection algorithm to obtain our contours.\n",
    "\n",
    "##### Step2:\n",
    "Approximation accuracy: The contour approximation algorithm requires a parameter called epsilon, which determines the approximation accuracy. This parameter controls the accuracy of contour approximation. A smaller value of epsilon will result in a more accurate approximation but with more points. While larger epsilon will result in a less accurate approximation with fewer points\n",
    "\n",
    "##### Step 3:\n",
    "Contour Approximation Algorithm: Algorithms such as the Douglas-Peucker Algorithm are used for contour approximation. It works by iteratively simplifying the contour using a distance threshold. The algorithm starts by identifying the point with maximum distance from the line segment connecting the first and last points of the contour. If the distance is above the threshold, that point is considered significant and kept as part of the simplified contour. The algorithm then recursively applies this process to the two resulting segments until the entire contour is approximated.\n",
    "Once the contour approximation is done, additional post-processing steps can be performed depending on the specific application. This may include filtering out small contours or performing additional analysis on the detected contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3880146",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 7/box_blade.jpg')\n",
    "image_copy = image.copy()\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, threshold = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#Find the Contours\n",
    "contours, _ = cv2.findContours(threshold, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE )\n",
    "\n",
    "for contour in contours:\n",
    "    # Perform contour approximation\n",
    "    epsilon = 0.05 * cv2.arcLength(contour, True)\n",
    "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "    \n",
    "    # Draw the contour and its approximation\n",
    "    cv2.drawContours(image, [approx], 0, (0, 0, 0), 1)\n",
    "    cv2.drawContours(image_copy, [contour], 0, (0, 0, 0), 1)\n",
    "    \n",
    "cv2.imshow('Contour Approximation', image)\n",
    "cv2.imshow('Contour Approximation 2', image_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834666d",
   "metadata": {},
   "source": [
    "### Contour Filtering and Selection\n",
    "Contour filtering and selection is a topic that explores techniques to refine and select contours based on specific criteria. It involves applying filters to eliminate unwanted contours and focusing on the most relevant or significant contours for further analysis or processing.\n",
    "By applying filters based on properties like area, perimeter, aspect ratio or shape characteristics, we can effectively narrow down the set of contours to those that meet our desired criteria. This allows us to extract and work with the contours that are most relevant to our application, enabling more accurate and targeted analysis of objects or regions of interest within an image.\n",
    "Some of the criteria that can be used for contour filtering and selection are as follows:\n",
    "##### Area-based filtering :\n",
    "This technique involves filtering contours based on their area. Contours with areas below or above certain thresholds can be discarded, helping to remove noise or small irrelevant regions.\n",
    "##### Perimeter-based filtering:\n",
    "This technique filters contours based on their perimeter. Contours with perimeter outside a specified range can be removed, effectively eliminating contours that are too small or too large. \n",
    "##### Aspect-Ratio filtering:\n",
    "Contours can be filtered based on their aspect ratio and is useful for selecting contours that are exhibit specific elongation or compactness characteristics.\n",
    "##### Shape Property Filtering:\n",
    "Technique using various other properties discussed earlier such as convex hull, extent and solidity as well to filter contours. Filtering can be done on contours based on these properties allowing for selecting contours that meet specific shape criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f79159f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 7/filter.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Find Contours\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#filter contours based on area\n",
    "filtered_contours = []\n",
    "filtered_objects = []\n",
    "for contour in contours:\n",
    "    area = cv2.contourArea(contour)\n",
    "    if area > 1000:\n",
    "        filtered_contours.append(contour)\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "       \n",
    "        filtered_objects.append(image[y:y+h, x:x+w])\n",
    "        \n",
    "print(len(filtered_contours))\n",
    "\n",
    "# Create a blank image of the same size as the original image\n",
    "result = np.zeros_like(image)\n",
    "\n",
    "# Draw the filtered contours on the result image\n",
    "cv2.drawContours(result, filtered_contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('Filtered Contours', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "for i, obj in enumerate(filtered_objects):\n",
    "    cv2.imshow(f'Object {i+1}', obj)\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cb4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
