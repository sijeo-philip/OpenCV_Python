{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49988a6",
   "metadata": {},
   "source": [
    "## FAST( Features from Accelerated Segment Test)\n",
    "Fast and Popular and widely used keypoint detection algorithm used in computer vision and image processing. The algorithm is designed to identify the corners or the key points in an image quickly with high accuracy. The algorithm works by analyzing the intensity variation in pixels of an image to detect key points or corners\n",
    "The algorithm works by taking a pixel as a candidate for a corner pixel and analyzing the points in its neighborhood. The intensity values of the center pixel are compared with these points in the neighborhood pixel as lighter or darker. A score is calculated based on these values and is used to classify if the pixel in consideration is a corner pixel or not. \n",
    "\n",
    "One of the main advantage of using the FAST algorithm is in computational efficiency. The algorithm is able to deliver real-time performance as it minimizes the number of intensity comparisons required in its implementation and is thus a suitable algorithm for use in various applications such as object recognition and image stitching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04309d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa41aa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 9/image.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Fast detector object\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# Detect keypoints using FAST\n",
    "keypoints = fast.detect(gray, None)\n",
    "\n",
    "# Draw Detected Keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0))\n",
    "\n",
    "cv2.imwrite('output.jpg', image_with_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293a2bd",
   "metadata": {},
   "source": [
    "## Harris Keypoint Detection\n",
    "Harris Corner detection is popular algorithm used to identify keypoint locations in an image. It is based  on the concept of corner points, which are areas where the image intensity changes significantly in multiple directions.\n",
    "These corner points are considered distinctive features that can be used for computer vision tasks, such as image matching and object recognition.\n",
    "\n",
    "The Harris Keypoint Detector involves the following steps.\n",
    "The following steps are involved:\n",
    "\n",
    "***1.Gradient Calculation:***  An image gradient operator such as the Sobel operator is applied to compute image gradients orientation and magnitudes.\n",
    "\n",
    "***2.Structure Tensor Calculation:***    The structure tensor is generated by calculating the products of the gradients at each pixel and summing them over local neighborhood. It provides information about image's local structure and orientation.\n",
    "\n",
    "***3.Corner Response Calculation:***   The corner response is computed using the structure tensor. It measures the likelihood of a pixel being a corner point based on the variations in intensity and gradient direction. It is calculated using the eigenvalues of the structure tensor. High eigenvalues indicate corners, while low eigenvalues represent flat or edge regions.\n",
    "\n",
    "***4.Non-Max Suppression:*** To eliminate multiple coner responses in close proximity, non-maximum suppression is applied. This step ensures that only the strongest corner responses are selected as keypoints. \n",
    "\n",
    "***5.Thresholding:***    Finally, a threshold is applied to the corner responses to filter out weak corners. Only corners with response values above a certain threshold are considered as keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b82db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 9/image_shoes.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Harris corner detector parameters\n",
    "block_size = 2\n",
    "ksize = 3\n",
    "k = 0.04\n",
    "\n",
    "# Harris corner detection\n",
    "corners = cv2.cornerHarris(gray, block_size, ksize, k)\n",
    "\n",
    "# Threshold and mark the detected corners\n",
    "threshold = 0.01 * corners.max()\n",
    "marked_image = image.copy()\n",
    "marked_image[corners > threshold] = [0, 0, 225]\n",
    "\n",
    "cv2.imwrite('output_shoes.jpg', marked_image)\n",
    "cv2.imshow('Output Image', marked_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c67333",
   "metadata": {},
   "source": [
    "## BRIEF (Binary Robust Independent Elementary Features)\n",
    "**BRIEF** is a binary descriptor used in computer vision for matching features of images. The algorithm focuses on describing key points or feature such as corners or edges in an image. \n",
    "The goal of BRIEF is to create a simple and compact representation of these features, known as descriptors, which can be compared and matched efficiently. BRIEF achieves this by comparing pairs of pixel intensities within a localize patch around each key point. The result of these comparisons is a binary code or string that represents the feature descriptor. This binary string captures the spatial relationships between the pixel pairs and encodes the distinctive characteristics of the feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e2493",
   "metadata": {},
   "source": [
    "The algorithm is as follows:\n",
    "***Keypoints:***    BRIEF uses keypoint detectors such as FAST to retrieve the key points on the image. A Guassian blue is applied around the keypoint to smoothen the image and remove any noise. Smoothing of the image is important as this can significantly improve performance of the BRIEF algorithm.\n",
    "\n",
    "***Pixel Pairs:***    The next step involves choosing multiple points  pairs(x, y) from the image to create a feature vector. These points are chosen at random, and n pixel pairs are selected to form our feature vector.\n",
    "The random points can be chosen using one of the following methods:\n",
    "\n",
    "***> Uniform Sampling:***    In Uniform sampling, each pixel pair(x and y) is selected randomly and uniformly from the keypoint region without any specific pattern or bias.\n",
    "\n",
    "***> Gaussian Sampling:*** A Gaussian distribution is used to sample pixel pairs (both x and y) from the keypoint region. The pixel locations are chosen based on a gaussian distribution centered around the keypoint. Pixel X and y are chosen using a Gaussian distribution with a standard deviation of ***0.04*N^2***, where N reperesents the patch size, around the key point. \n",
    "\n",
    "***> Gaussian Sampling(II):*** In this method, both points are sampled seperately. The x-coordinate is sampled using a Gaussian distribution with a standard deviation of ***0.04*N^2***, where N represents the patch size. the y-coordinate is then sampled using a Gaussian distribution centered around the X coordinate with a standard deviation of ***0.01*N^2*** In this method, both points are sampled seperately. The X-coordinate is sampled using a Gaussian distribution with a standard deviation of ***0.04*N^2***, where N represents the patch size. the Y-coordinate is then sampled using Gaussian distribution centered around the x-coordinate with standard deviation of ***0.01 * N^2***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0dadb",
   "metadata": {},
   "source": [
    "***Coarse Polar Grid Sampling:***    In this method, the x and y point locations are randomly sampled from a coarse polar grid, introducing spatial quantization. A coarse polar grid is like a grid of predefined circles and lines that help us pick specific points in a circular area around a object or point of interest. The grid represents discrete locations, and the sampling is perfromed within the grid structure.\n",
    "\n",
    "***Centered Polar Grid Sampling:*** In this method, the x pixel is fixed at (0,0) and the y is sampled across a polar grid to cover all possible values.\n",
    "\n",
    "***Intensity Comparison:*** Now that we have n pixel pairs, the algorithm uses these pixel pairs to compare intensity values between them. In each pixel pair, the intensity value is extracted for each pixel and compared to the other pixel in the pair.\n",
    "\n",
    "Based on the intensity comparison results, we generate a binary string or descriptor. Each element of the binary descriptor corresponds to the result of a specific intensity comparison between the two patches. If the intensity at $x_i$ is greater than the intensity at $y_i$, the corresponding element of the binary descriptor is set to 1; otherwise, it is set to 0.\n",
    "\n",
    "The end result is a vector consisting of 1s and 0s representing the output from the binary descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588acd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('Images/Input Images/Chapter 9/dog.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image2 = cv2.imread('Images/Input Images/Chapter 9/dog2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Initialize the ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "#Set the ORB score type to BRIEF\n",
    "orb.setScoreType(cv2.ORB_FAST_SCORE)\n",
    "\n",
    "#Detect Keypoints and compute descriptors using ORB\n",
    "keypoints1, descriptor1 = orb.detectAndCompute(image1, None)\n",
    "keypoints2, descriptor2 = orb.detectAndCompute(image2, None)\n",
    "\n",
    "#Create a BFMatcher object\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = matcher.match(descriptor1, descriptor2)\n",
    "\n",
    "# Sort matches by score\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "#Draw top matches\n",
    "matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imwrite('brief.jpg', matched_image)\n",
    "cv2.imshow(\"Output Image\", matched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d630b1",
   "metadata": {},
   "source": [
    "## ORB (Oriented FAST and Rotated BRIEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a1a6f",
   "metadata": {},
   "source": [
    "ORB is a feature detection and description algorithm than combines the FAST algorithm for keypoint detection and BRIEF algorithm for feature description.\n",
    "\n",
    "***1.Keypoint Dectection:***    The algorithm firest uses the FAST algorithm to detect key points in an image. After applying the FAST algorithm to identify potential keypoints, ORB applies the Harris corner measure to evaluate the quality of each detected corner. The Harris Corner measures assesses the local image structure around each corner candidate and assigns a score based on the corner's uniqueness and saliency. ORB then selects the top N points with the highest scores among the corner candidates, ensuring that onlly the most relevant keypoints are retained for further processing.\n",
    "\n",
    "Additionally, a pyramid approach is used to generate key points at multiple scales, enabling the detection key points across different levels of image details.\n",
    "\n",
    "***2. Orientation Assignment:***   This step is introduced to add rotational invariance in the algorithm. The step computes the intensity weighted centroid of the patch with the located corner ath the center. By determining the direction of the vector from the corner point of the centroid, the orientation of the keypoint is established. \n",
    "\n",
    "Moreover, moments are computed using x and y coordinates within a circular region of radius r, which corresponds to the size of the patch.\n",
    "\n",
    "***3. BRIEF Descriptor:***    Once the orientations are assigned, the BRIEF descriptor computation takes place. As discussed earlier, in the BRIEF descriptor, random pixel pairs within the patch centered at each keypoint are selected, and the intensities of these pairs are compared. The result is a binary descriptor that captures the local image features around the keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79e943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009610f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image =cv2.imread('Images/Input Images/Chapter 9/image_house.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Initialize  ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "\n",
    "#Draw the Keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=0)\n",
    "\n",
    "cv2.imwrite('orb.jpg', image_with_keypoints)\n",
    "cv2.imshow('ORB_Image', image_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efaee4e",
   "metadata": {},
   "source": [
    "## SIFT( Scale-Invariant Feature Transform)\n",
    "SIFT is a widely used feature detection and description  algorithm used in computer vision and image processing. SIFT is a local invariant descriptor, implying that it describes local image features that are invariant to changes in image transformations such as rotation and image scaling.\n",
    "\n",
    "The SIFT algorithm is also robust to changes in lighting and can handle noise and the occulsion in an image, making it a popular algorithm to be used for various application such as object recognition, image matching and image stitching amongst many others. \n",
    "\n",
    "The SIFT algorithm works by following the given steps:\n",
    "***1. Scale Space Extrema Detection:*** The SIFT algorithm starts by creating a scale space pyramid. The scale space pyramid is a series of images obtained by applying the guassian blurring and downsampling repeatedly, effectively capturing the image at different scales:\n",
    "\n",
    "The ***Difference of Guassian(DoG)*** pyramid is constructed by taking the difference of adjacent scale images from the pyramid. This pyramid highlights the areas with significant intensity changes in an image, representing the significant key points in an image.\n",
    "A pixel in an image is compared to its neighbouring 8 points and 9 points each from the previous and next scales. Keypoint candidates are identified as local extrema in the DoG pyramid, where a pixel has a higher or lower intensity compared to  its neighbouring pixels in both scale and space.\n",
    "\n",
    "***2. Keypoint Localization:*** once the potential keypoints are found, they nedd to be refined to ensure only the stable key points remain in our image. The keypoints are refined by fitting a quadratic function to the nearby DoG samples, which helps to increase the accuracy of the keypoint positions.\n",
    "The process starts by considering each potential keypoint and defining a local neighborhood around it. Within this region, SIFT fits a quadratic function to understand how the brightness and changes in brightness occur. This curve allows SIFT to pinpoint the precise location of the keypoint, even if it falls between two pixels, increasing accuracy. \n",
    "Additionally, keypoints with low contrast or located on edges are eliminated to ensure than only distinctive and stable keypoints are retained. \n",
    "\n",
    "***3. Orientation Assignment:*** Now an orientation is assigned to each keypoint in the image to achieve rotational invariance. \n",
    "To do this, a neighborhood is taken around the keypoint and gradient magnitudes and orientations are computed. SIFT constructs a histogram to capter the distribution of gradient orientations within the neighborhood. SIFT observes which direction changes happen (like getting brighter or darker ) and how much they change in that direction. \n",
    "\n",
    "It creates a special chart with 36 bars representing all the possible directions in a full circle (360 degrees). An orientation histogram with 36 bins covering 360 degrees is created. \n",
    "\n",
    "The highest peak in the histogram indicates the most common orientation. SIFT selects that direction as the orientation for the keypoint. \n",
    "\n",
    "***4. Keypoint Descriptor:***    A descriptor is now created to describe each keypoint to capture its local appearance. A 16x16 neighborhood around the keypoint is considered and the area is divided into 16 sub-blocks of 4x4 size.\n",
    "For each subregion, SIFT calculates gradient orientations and magnitudes. These gradient values are then weighted by a Guassian function centered at the keypoint location. The 16 histograms of 8 bins each are then concatenated to form a 128-dimensional feature vector. This vector is then normalized to  improve its robustness to changes in illuminations and contrast. \n",
    "\n",
    "Output: This 128 feature vector can now be used for applications such as image matching by comparing it to other SIFT vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e3e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59fa2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('Images/Input Images/Chapter 9/dog.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image2 = cv2.imread('Images/Input Images/Chapter 9/dog2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Initialize the SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "#Detect Keypoints and compute descriptors\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n",
    "\n",
    "#Create a BFMatcher Object\n",
    "matcher = cv2.BFMatcher()\n",
    "\n",
    "#Match Descriptors\n",
    "matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "#Apply ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "        \n",
    "#Draw matches\n",
    "matched_images = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#Display the matched images\n",
    "cv2.imwrite('output_matches.jpg', matched_images)\n",
    "cv2.imshow('Output Match Image', matched_images)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe2a32",
   "metadata": {},
   "source": [
    "## RootSIFT( Root Scale-Invariant Feature Transform)\n",
    "RootSIFT is a variation of the SIFT algorithm that enchances the original SIFT descriptor by applying an additional normalization step.\n",
    "\n",
    "IN the SIFT Algorithm, we saw that the final descriptor is created by taking the Gradient magnitudes and orientations in the image. However the gradients can sometimes vary in different images overall affecting the performance of the algorithm. To address this issue, RootSIFT takes the square root of the gradient magnitudes overall, reducing the effect of large gradient magnitudes on the output.\n",
    "\n",
    "The final vector is then normalized for introducing intensity invariance and can now be used for various computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf32497",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('Images/Input Images/Chapter 9/2211.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image2 = cv2.imread('Images/Input Images/Chapter 9/22.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Rotate Image\n",
    "angle = 45\n",
    "rows, cols = image1.shape[:2]\n",
    "rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
    "rotated_image = cv2.warpAffine(image1, rotation_matrix, (cols, rows))\n",
    "\n",
    "#Intialize the SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect Keypoints and compute descriptors\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n",
    "\n",
    "#Compute RootSIFT descriptors\n",
    "epsilon = 1e-7   # Small constant to avoid division by zero\n",
    "descriptors1 /= (descriptors1.sum(axis=1, keepdims=True) + epsilon)\n",
    "descriptors1 = np.sqrt(descriptors1)\n",
    "\n",
    "descriptors2 /= (descriptors2.sum(axis=1, keepdims=True) + epsilon)\n",
    "descriptors2 = np.sqrt(descriptors2)\n",
    "\n",
    "#Create a BFMatcher object\n",
    "matcher = cv2.BFMatcher()\n",
    "\n",
    "#Match the descriptors\n",
    "matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "#Filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "        \n",
    "good_matches = good_matches[:15]\n",
    "\n",
    "#Draw Matches\n",
    "matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.imwrite('root.jpg', matched_image)\n",
    "cv2.imshow(\"RootSIFT Output\", matched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0e1fe",
   "metadata": {},
   "source": [
    "## SURF (Speeded-UP Robust Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399e5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
