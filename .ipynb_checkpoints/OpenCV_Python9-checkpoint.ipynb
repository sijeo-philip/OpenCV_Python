{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49988a6",
   "metadata": {},
   "source": [
    "## FAST( Features from Accelerated Segment Test)\n",
    "Fast and Popular and widely used keypoint detection algorithm used in computer vision and image processing. The algorithm is designed to identify the corners or the key points in an image quickly with high accuracy. The algorithm works by analyzing the intensity variation in pixels of an image to detect key points or corners\n",
    "The algorithm works by taking a pixel as a candidate for a corner pixel and analyzing the points in its neighborhood. The intensity values of the center pixel are compared with these points in the neighborhood pixel as lighter or darker. A score is calculated based on these values and is used to classify if the pixel in consideration is a corner pixel or not. \n",
    "\n",
    "One of the main advantage of using the FAST algorithm is in computational efficiency. The algorithm is able to deliver real-time performance as it minimizes the number of intensity comparisons required in its implementation and is thus a suitable algorithm for use in various applications such as object recognition and image stitching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04309d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa41aa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 9/image.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Fast detector object\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# Detect keypoints using FAST\n",
    "keypoints = fast.detect(gray, None)\n",
    "\n",
    "# Draw Detected Keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0))\n",
    "\n",
    "cv2.imwrite('output.jpg', image_with_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293a2bd",
   "metadata": {},
   "source": [
    "## Harris Keypoint Detection\n",
    "Harris Corner detection is popular algorithm used to identify keypoint locations in an image. It is based  on the concept of corner points, which are areas where the image intensity changes significantly in multiple directions.\n",
    "These corner points are considered distinctive features that can be used for computer vision tasks, such as image matching and object recognition.\n",
    "\n",
    "The Harris Keypoint Detector involves the following steps.\n",
    "The following steps are involved:\n",
    "\n",
    "***1.Gradient Calculation:***  An image gradient operator such as the Sobel operator is applied to compute image gradients orientation and magnitudes.\n",
    "\n",
    "***2.Structure Tensor Calculation:***    The structure tensor is generated by calculating the products of the gradients at each pixel and summing them over local neighborhood. It provides information about image's local structure and orientation.\n",
    "\n",
    "***3.Corner Response Calculation:***   The corner response is computed using the structure tensor. It measures the likelihood of a pixel being a corner point based on the variations in intensity and gradient direction. It is calculated using the eigenvalues of the structure tensor. High eigenvalues indicate corners, while low eigenvalues represent flat or edge regions.\n",
    "\n",
    "***4.Non-Max Suppression:*** To eliminate multiple coner responses in close proximity, non-maximum suppression is applied. This step ensures that only the strongest corner responses are selected as keypoints. \n",
    "\n",
    "***5.Thresholding:***    Finally, a threshold is applied to the corner responses to filter out weak corners. Only corners with response values above a certain threshold are considered as keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b82db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/Input Images/Chapter 9/image_shoes.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Harris corner detector parameters\n",
    "block_size = 2\n",
    "ksize = 3\n",
    "k = 0.04\n",
    "\n",
    "# Harris corner detection\n",
    "corners = cv2.cornerHarris(gray, block_size, ksize, k)\n",
    "\n",
    "# Threshold and mark the detected corners\n",
    "threshold = 0.01 * corners.max()\n",
    "marked_image = image.copy()\n",
    "marked_image[corners > threshold] = [0, 0, 225]\n",
    "\n",
    "cv2.imwrite('output_shoes.jpg', marked_image)\n",
    "cv2.imshow('Output Image', marked_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c67333",
   "metadata": {},
   "source": [
    "## BRIEF (Binary Robust Independent Elementary Features)\n",
    "**BRIEF** is a binary descriptor used in computer vision for matching features of images. The algorithm focuses on describing key points or feature such as corners or edges in an image. \n",
    "The goal of BRIEF is to create a simple and compact representation of these features, known as descriptors, which can be compared and matched efficiently. BRIEF achieves this by comparing pairs of pixel intensities within a localize patch around each key point. The result of these comparisons is a binary code or string that represents the feature descriptor. This binary string captures the spatial relationships between the pixel pairs and encodes the distinctive characteristics of the feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e2493",
   "metadata": {},
   "source": [
    "The algorithm is as follows:\n",
    "***Keypoints:***    BRIEF uses keypoint detectors such as FAST to retrieve the key points on the image. A Guassian blue is applied around the keypoint to smoothen the image and remove any noise. Smoothing of the image is important as this can significantly improve performance of the BRIEF algorithm.\n",
    "\n",
    "***Pixel Pairs:***    The next step involves choosing multiple points  pairs(x, y) from the image to create a feature vector. These points are chosen at random, and n pixel pairs are selected to form our feature vector.\n",
    "The random points can be chosen using one of the following methods:\n",
    "\n",
    "***> Uniform Sampling:***    In Uniform sampling, each pixel pair(x and y) is selected randomly and uniformly from the keypoint region without any specific pattern or bias.\n",
    "\n",
    "***> Gaussian Sampling:*** A Gaussian distribution is used to sample pixel pairs (both x and y) from the keypoint region. The pixel locations are chosen based on a gaussian distribution centered around the keypoint. Pixel X and y are chosen using a Gaussian distribution with a standard deviation of ***0.04*N^2***, where N reperesents the patch size, around the key point. \n",
    "\n",
    "***> Gaussian Sampling(II):*** In this method, both points are sampled seperately. The x-coordinate is sampled using a Gaussian distribution with a standard deviation of ***0.04*N^2***, where N represents the patch size. the y-coordinate is then sampled using a Gaussian distribution centered around the X coordinate with a standard deviation of ***0.01*N^2*** In this method, both points are sampled seperately. The X-coordinate is sampled using a Gaussian distribution with a standard deviation of ***0.04*N^2***, where N represents the patch size. the Y-coordinate is then sampled using Gaussian distribution centered around the x-coordinate with standard deviation of ***0.01 * N^2***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0dadb",
   "metadata": {},
   "source": [
    "***Coarse Polar Grid Sampling:***    In this method, the x and y point locations are randomly sampled from a coarse polar grid, introducing spatial quantization. A coarse polar grid is like a grid of predefined circles and lines that help us pick specific points in a circular area around a object or point of interest. The grid represents discrete locations, and the sampling is perfromed within the grid structure.\n",
    "\n",
    "***Centered Polar Grid Sampling:*** In this method, the x pixel is fixed at (0,0) and the y is sampled across a polar grid to cover all possible values.\n",
    "\n",
    "***Intensity Comparison:*** Now that we have n pixel pairs, the algorithm uses these pixel pairs to compare intensity values between them. In each pixel pair, the intensity value is extracted for each pixel and compared to the other pixel in the pair.\n",
    "\n",
    "Based on the intensity comparison results, we generate a binary string or descriptor. Each element of the binary descriptor corresponds to the result of a specific intensity comparison between the two patches. If the intensity at $x_i$ is greater than the intensity at $y_i$, the corresponding element of the binary descriptor is set to 1; otherwise, it is set to 0.\n",
    "\n",
    "The end result is a vector consisting of 1s and 0s representing the output from the binary descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588acd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread('Images/Input Images/Chapter 9/dog.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image2 = cv2.imread('Images/Input Images/Chapter 9/dog2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Initialize the ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "#Set the ORB score type to BRIEF\n",
    "orb.setScoreType(cv2.ORB_FAST_SCORE)\n",
    "\n",
    "#Detect Keypoints and compute descriptors using ORB\n",
    "keypoints1, descriptor1 = orb.detectAndCompute(image1, None)\n",
    "keypoints2, descriptor2 = orb.detectAndCompute(image2, None)\n",
    "\n",
    "#Create a BFMatcher object\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = matcher.match(descriptor1, descriptor2)\n",
    "\n",
    "# Sort matches by score\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "#Draw top matches\n",
    "matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imwrite('brief.jpg', matched_image)\n",
    "cv2.imshow(\"Output Image\", matched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d630b1",
   "metadata": {},
   "source": [
    "## ORB (Oriented FAST and Rotated BRIEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01042e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
